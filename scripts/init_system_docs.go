//go:build ignore

package main

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"

	config "hunt-chat-api/configs"
	"hunt-chat-api/pkg/services"

	"github.com/google/uuid"
	"github.com/joho/godotenv"
)

func main() {
	log.Println("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’é–‹å§‹ã—ã¾ã™...")

	// .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
	if err := godotenv.Load(); err != nil {
		log.Printf("Warning: .env file not found: %v", err)
	}

	// è¨­å®šã‚’èª­ã¿è¾¼ã‚€
	cfg := config.LoadConfig()

	// OpenAI ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
	openaiService := services.NewAzureOpenAIService(
		cfg.AzureOpenAIEndpoint,
		cfg.AzureOpenAIAPIKey,
		cfg.AzureOpenAIAPIVersion,
		cfg.AzureOpenAIChatDeploymentName,
		cfg.AzureOpenAIEmbeddingDeploymentName,
	)

	// VectorStore ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
	vectorStoreService, err := services.NewVectorStoreService(openaiService, cfg.QdrantURL, cfg.QdrantAPIKey)
	if err != nil {
		log.Fatalf("VectorStoreã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–ã«å¤±æ•—: %v", err)
	}

	ctx := context.Background()

	// ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‚’å›ºå®š
	const collectionName = "hunt_documents"

	// å…ˆã«æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã™ã¹ã¦å‰Šé™¤
	log.Printf("ğŸ—‘ï¸ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '%s' ã‹ã‚‰æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã™...", collectionName)
	if err := vectorStoreService.DeleteDocumentsByType(ctx, collectionName, "system_documentation"); err != nil {
		// ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã‚’ç¶šè¡Œã™ã‚‹ï¼ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆãªã©ï¼‰
		log.Printf("æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‰Šé™¤ä¸­ã«è­¦å‘Š: %v", err)
	} else {
		log.Println("âœ… æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
	}

	// ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆæ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«å¯¾å¿œï¼‰
	docs := []string{
		"README.md",
		"docs/api/API_MANUAL.md",
		"docs/guides/FILE_FORMAT_GUIDE.md",
		"docs/guides/AI_LEARNING_GUIDE.md",
		"docs/features/AI_QUESTION_STRATEGY.md",
		"docs/implementation/AI_QUESTION_IMPLEMENTATION.md",
		"docs/implementation/IMPLEMENTATION_SUMMARY.md",
		"docs/implementation/FINAL_IMPLEMENTATION_SUMMARY.md",
		"docs/implementation/ECONOMIC_CORRELATION_IMPLEMENTATION.md",
		"docs/features/CHAT_HISTORY_RAG.md",
		"docs/guides/WEEKLY_ANALYSIS_GUIDE.md",
		"docs/guides/DATA_AGGREGATION_GUIDE.md",
		"docs/guides/RAG_SYSTEM_GUIDE.md",
		"docs/guides/TROUBLESHOOTING_AND_BEST_PRACTICES.md",
		"docs/architecture/UML.md",
		"docs/project/è¦ä»¶å®šç¾©.md",
		"docs/project/ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼.md",
	}

	successCount := 0
	failCount := 0

	for _, docName := range docs {
		log.Printf("ğŸ“„ å‡¦ç†ä¸­: %s", docName)

		// ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
		content, err := os.ReadFile(docName)
		if err != nil {
			log.Printf("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: %s - %v", docName, err)
			failCount++
			continue
		}

		docText := string(content)

		// é•·ã„æ–‡æ›¸ã‚’åˆ†å‰² (ç´„6000æ–‡å­—ã”ã¨ã€å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’è€ƒæ…®)
		maxChunkSize := 6000
		chunks := splitDocument(docText, maxChunkSize)

		log.Printf("  ğŸ“¦ %då€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²", len(chunks))

		chunkSuccess := 0
		for i, chunk := range chunks {
			// ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰æ±ºå®šè«–çš„ã«ç”Ÿæˆ
			idSource := fmt.Sprintf("%s-chunk-%d", docName, i)
			documentID := uuid.NewSHA1(uuid.NameSpaceURL, []byte(idSource)).String()

			metadata := map[string]interface{}{
				"type":         "system_documentation",
				"file_name":    docName,
				"category":     getDocCategory(docName),
				"description":  getDocDescription(docName),
				"chunk_index":  i,
				"total_chunks": len(chunks),
			}

			err = vectorStoreService.StoreDocument(ctx, collectionName, documentID, chunk, metadata)
			if err != nil {
				log.Printf("  âš ï¸ ãƒãƒ£ãƒ³ã‚¯ %d/%d ã®ä¿å­˜ã«å¤±æ•—: %v", i+1, len(chunks), err)
				continue
			}
			chunkSuccess++
		}

		if chunkSuccess == len(chunks) {
			log.Printf("âœ… ä¿å­˜æˆåŠŸ: %s (%d/%d ãƒãƒ£ãƒ³ã‚¯)", docName, chunkSuccess, len(chunks))
			successCount++
		} else if chunkSuccess > 0 {
			log.Printf("âš ï¸ éƒ¨åˆ†çš„ã«æˆåŠŸ: %s (%d/%d ãƒãƒ£ãƒ³ã‚¯)", docName, chunkSuccess, len(chunks))
			successCount++
		} else {
			log.Printf("âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¿å­˜ã«å¤±æ•—: %s - ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ãŒå¤±æ•—", docName)
			failCount++
		}
	}

	separator := strings.Repeat("=", 50)
	log.Println("\n" + separator)
	log.Printf("ğŸ“Š åˆæœŸåŒ–å®Œäº†")
	log.Printf("  æˆåŠŸ: %dä»¶", successCount)
	log.Printf("  å¤±æ•—: %dä»¶", failCount)
	log.Println(separator)

	if failCount > 0 {
		os.Exit(1)
	}
}

// getDocCategory ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™
func getDocCategory(filename string) string {
	// æ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«åŸºã¥ã„ã¦ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®š
	if strings.Contains(filename, "docs/api/") {
		return "api"
	} else if strings.Contains(filename, "docs/architecture/") {
		return "architecture"
	} else if strings.Contains(filename, "docs/implementation/") {
		return "implementation"
	} else if strings.Contains(filename, "docs/guides/") {
		return "guides"
	} else if strings.Contains(filename, "docs/features/") {
		return "features"
	} else if strings.Contains(filename, "docs/project/") {
		return "project"
	}
	return "general"
}

// getDocDescription ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª¬æ˜ã‚’è¿”ã™
func getDocDescription(filename string) string {
	descriptions := map[string]string{
		"README.md":                                                  "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¦‚è¦ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †",
		"docs/api/API_MANUAL.md":                                     "APIåˆ©ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ« - ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ä½¿ç”¨æ–¹æ³•",
		"docs/guides/FILE_FORMAT_GUIDE.md":                           "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å½¢å¼ã‚¬ã‚¤ãƒ‰ - å¿…é ˆåˆ—ã¨å½¢å¼ã®è©³ç´°",
		"docs/guides/AI_LEARNING_GUIDE.md":                           "AIå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¬ã‚¤ãƒ‰ - å›ç­”ä¿å­˜ã¨æ´å¯Ÿå–å¾—",
		"docs/features/AI_QUESTION_STRATEGY.md":                      "AIè³ªå•åŠ›å‘ä¸Šæˆ¦ç•¥ã‚¬ã‚¤ãƒ‰ - å¤šå±¤è³ªå•ã€ã‚·ãƒŠãƒªã‚ªãƒ™ãƒ¼ã‚¹ã€ç¶™ç¶šè³ªå•ã®è¨­è¨ˆ",
		"docs/implementation/AI_QUESTION_IMPLEMENTATION.md":          "AIè³ªå•æ©Ÿèƒ½ã®å®Ÿè£…ã‚µãƒ³ãƒ—ãƒ« - å¼·åŒ–è³ªå•ã€ä»®èª¬ç”Ÿæˆã€å›ç­”å“è³ªåˆ†æã®ã‚³ãƒ¼ãƒ‰",
		"docs/implementation/IMPLEMENTATION_SUMMARY.md":              "å®Ÿè£…æ¦‚è¦ - ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯",
		"docs/implementation/FINAL_IMPLEMENTATION_SUMMARY.md":        "æœ€çµ‚å®Ÿè£…ã‚µãƒãƒªãƒ¼ - ç›¸é–¢åˆ†ææœ€é©åŒ–ã¨RAGæ´»ç”¨",
		"docs/implementation/ECONOMIC_CORRELATION_IMPLEMENTATION.md": "çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ç›¸é–¢åˆ†æå®Ÿè£… - æ—¥çµŒå¹³å‡ã€ç‚ºæ›¿ã€åŸæ²¹ä¾¡æ ¼ã¨ã®ç›¸é–¢",
		"docs/features/CHAT_HISTORY_RAG.md":                          "ãƒãƒ£ãƒƒãƒˆå±¥æ­´RAGæ©Ÿèƒ½ã®èª¬æ˜",
		"docs/guides/WEEKLY_ANALYSIS_GUIDE.md":                       "è£½å“åˆ¥åˆ†ææ©Ÿèƒ½ã®ä½¿ã„æ–¹ï¼ˆæ—§ï¼šé€±æ¬¡åˆ†æï¼‰",
		"docs/guides/DATA_AGGREGATION_GUIDE.md":                      "ãƒ‡ãƒ¼ã‚¿é›†ç´„åˆ†æã‚¬ã‚¤ãƒ‰ - æ—¥æ¬¡ãƒ»é€±æ¬¡ãƒ»æœˆæ¬¡åˆ†æ",
		"docs/guides/RAG_SYSTEM_GUIDE.md":                            "RAGï¼ˆæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰ã‚·ã‚¹ãƒ†ãƒ ã‚¬ã‚¤ãƒ‰",
		"docs/guides/TROUBLESHOOTING_AND_BEST_PRACTICES.md":          "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
		"docs/architecture/UML.md":                                   "UMLå›³ã¨ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£",
		"docs/project/è¦ä»¶å®šç¾©.md":                                       "ã‚·ã‚¹ãƒ†ãƒ ã®è¦ä»¶å®šç¾©æ›¸",
		"docs/project/ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼.md":                                     "ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å›³",
	}

	if desc, ok := descriptions[filename]; ok {
		return desc
	}
	return "ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"
}

// splitDocument é•·ã„æ–‡æ›¸ã‚’æŒ‡å®šã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
func splitDocument(text string, maxSize int) []string {
	if len(text) <= maxSize {
		return []string{text}
	}

	var chunks []string
	lines := strings.Split(text, "\n")
	currentChunk := ""

	for _, line := range lines {
		// æ¬¡ã®è¡Œã‚’è¿½åŠ ã™ã‚‹ã¨åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆ
		if len(currentChunk)+len(line)+1 > maxSize && currentChunk != "" {
			chunks = append(chunks, currentChunk)
			currentChunk = line + "\n"
		} else {
			if currentChunk != "" {
				currentChunk += "\n"
			}
			currentChunk += line
		}
	}

	// æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
	if currentChunk != "" {
		chunks = append(chunks, currentChunk)
	}

	return chunks
}
