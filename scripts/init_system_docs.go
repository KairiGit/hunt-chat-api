package main

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"

	config "hunt-chat-api/configs"
	"hunt-chat-api/pkg/services"

	"github.com/google/uuid"
	"github.com/joho/godotenv"
)

func main() {
	log.Println("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’é–‹å§‹ã—ã¾ã™...")

	// .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
	if err := godotenv.Load(); err != nil {
		log.Printf("Warning: .env file not found: %v", err)
	}

	// è¨­å®šã‚’èª­ã¿è¾¼ã‚€
	cfg := config.LoadConfig()

	// OpenAI ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
	openaiService := services.NewAzureOpenAIService(
		cfg.AzureOpenAIEndpoint,
		cfg.AzureOpenAIAPIKey,
		cfg.AzureOpenAIAPIVersion,
		cfg.AzureOpenAIChatDeploymentName,
		cfg.AzureOpenAIEmbeddingDeploymentName,
	)

	// VectorStore ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
	vectorStoreService, err := services.NewVectorStoreService(openaiService, cfg.QdrantURL, cfg.QdrantAPIKey)
	if err != nil {
		log.Fatalf("VectorStoreã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–ã«å¤±æ•—: %v", err)
	}

	ctx := context.Background()

	// ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
	docs := []string{
		"README.md",
		"API_MANUAL.md",
		"FILE_FORMAT_GUIDE.md",
		"AI_LEARNING_GUIDE.md",
		"IMPLEMENTATION_SUMMARY.md",
		"CHAT_HISTORY_RAG.md",
		"WEEKLY_ANALYSIS_GUIDE.md",
		"TROUBLESHOOTING_AND_BEST_PRACTICES.md",
		"è¦ä»¶å®šç¾©.md",
		"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼.md",
		"UML.md",
	}

	successCount := 0
	failCount := 0

	for _, docName := range docs {
		log.Printf("ğŸ“„ å‡¦ç†ä¸­: %s", docName)

		// ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
		content, err := os.ReadFile(docName)
		if err != nil {
			log.Printf("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: %s - %v", docName, err)
			failCount++
			continue
		}

		// ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜
		collectionName := fmt.Sprintf("system_doc_%s", docName)
		docText := string(content)

		// é•·ã„æ–‡æ›¸ã‚’åˆ†å‰² (ç´„6000æ–‡å­—ã”ã¨ã€å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’è€ƒæ…®)
		maxChunkSize := 6000
		chunks := splitDocument(docText, maxChunkSize)

		log.Printf("  ğŸ“¦ %då€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²", len(chunks))

		// å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
		if err := vectorStoreService.DeleteDocumentByFileName(ctx, collectionName, docName); err != nil {
			log.Printf("  âš ï¸ å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‰Šé™¤ã«å¤±æ•—ï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰: %v", err)
		} else {
			log.Printf("  ğŸ—‘ï¸ å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
		}

		chunkSuccess := 0
		for i, chunk := range chunks {
			documentID := uuid.New().String() // æœ‰åŠ¹ãªUUIDã‚’ç”Ÿæˆ
			metadata := map[string]interface{}{
				"type":         "system_documentation",
				"file_name":    docName,
				"category":     getDocCategory(docName),
				"description":  getDocDescription(docName),
				"chunk_index":  i,
				"total_chunks": len(chunks),
			}

			err = vectorStoreService.StoreDocument(ctx, collectionName, documentID, chunk, metadata)
			if err != nil {
				log.Printf("  âš ï¸ ãƒãƒ£ãƒ³ã‚¯ %d/%d ã®ä¿å­˜ã«å¤±æ•—: %v", i+1, len(chunks), err)
				continue
			}
			chunkSuccess++
		}

		if chunkSuccess == len(chunks) {
			log.Printf("âœ… ä¿å­˜æˆåŠŸ: %s (%d/%d ãƒãƒ£ãƒ³ã‚¯)", docName, chunkSuccess, len(chunks))
			successCount++
		} else if chunkSuccess > 0 {
			log.Printf("âš ï¸ éƒ¨åˆ†çš„ã«æˆåŠŸ: %s (%d/%d ãƒãƒ£ãƒ³ã‚¯)", docName, chunkSuccess, len(chunks))
			successCount++
		} else {
			log.Printf("âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¿å­˜ã«å¤±æ•—: %s - ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ãŒå¤±æ•—", docName)
			failCount++
		}
	}

	separator := strings.Repeat("=", 50)
	log.Println("\n" + separator)
	log.Printf("ğŸ“Š åˆæœŸåŒ–å®Œäº†")
	log.Printf("  æˆåŠŸ: %dä»¶", successCount)
	log.Printf("  å¤±æ•—: %dä»¶", failCount)
	log.Println(separator)

	if failCount > 0 {
		os.Exit(1)
	}
}

// getDocCategory ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™
func getDocCategory(filename string) string {
	switch filename {
	case "API_MANUAL.md", "FILE_FORMAT_GUIDE.md":
		return "api"
	case "AI_LEARNING_GUIDE.md", "CHAT_HISTORY_RAG.md":
		return "ai"
	case "è¦ä»¶å®šç¾©.md", "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼.md", "UML.md":
		return "design"
	case "IMPLEMENTATION_SUMMARY.md", "TROUBLESHOOTING_AND_BEST_PRACTICES.md":
		return "development"
	case "WEEKLY_ANALYSIS_GUIDE.md":
		return "usage"
	default:
		return "general"
	}
}

// getDocDescription ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª¬æ˜ã‚’è¿”ã™
func getDocDescription(filename string) string {
	descriptions := map[string]string{
		"README.md":                             "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¦‚è¦ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †",
		"API_MANUAL.md":                         "APIåˆ©ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ« - ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ä½¿ç”¨æ–¹æ³•",
		"FILE_FORMAT_GUIDE.md":                  "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å½¢å¼ã‚¬ã‚¤ãƒ‰ - å¿…é ˆåˆ—ã¨å½¢å¼ã®è©³ç´°",
		"AI_LEARNING_GUIDE.md":                  "AIå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¬ã‚¤ãƒ‰ - å›ç­”ä¿å­˜ã¨æ´å¯Ÿå–å¾—",
		"IMPLEMENTATION_SUMMARY.md":             "å®Ÿè£…æ¦‚è¦ - ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯",
		"CHAT_HISTORY_RAG.md":                   "ãƒãƒ£ãƒƒãƒˆå±¥æ­´RAGæ©Ÿèƒ½ã®èª¬æ˜",
		"WEEKLY_ANALYSIS_GUIDE.md":              "é€±æ¬¡åˆ†ææ©Ÿèƒ½ã®ä½¿ã„æ–¹",
		"TROUBLESHOOTING_AND_BEST_PRACTICES.md": "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
		"è¦ä»¶å®šç¾©.md":                               "ã‚·ã‚¹ãƒ†ãƒ ã®è¦ä»¶å®šç¾©æ›¸",
		"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼.md":                             "ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å›³",
		"UML.md":                                "UMLå›³ã¨ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ",
	}

	if desc, ok := descriptions[filename]; ok {
		return desc
	}
	return "ã‚·ã‚¹ãƒ†ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"
}

// splitDocument é•·ã„æ–‡æ›¸ã‚’æŒ‡å®šã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
func splitDocument(text string, maxSize int) []string {
	if len(text) <= maxSize {
		return []string{text}
	}

	var chunks []string
	lines := strings.Split(text, "\n")
	currentChunk := ""

	for _, line := range lines {
		// æ¬¡ã®è¡Œã‚’è¿½åŠ ã™ã‚‹ã¨åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆ
		if len(currentChunk)+len(line)+1 > maxSize && currentChunk != "" {
			chunks = append(chunks, currentChunk)
			currentChunk = line + "\n"
		} else {
			if currentChunk != "" {
				currentChunk += "\n"
			}
			currentChunk += line
		}
	}

	// æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
	if currentChunk != "" {
		chunks = append(chunks, currentChunk)
	}

	return chunks
}
