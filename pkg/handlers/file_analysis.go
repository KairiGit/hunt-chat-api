package handlers

import (
	"bytes"
	"context"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"log"
	"math"
	"net/http"
	"sort"
	"strconv"
	"strings"
	"sync"
	"time"

	"hunt-chat-api/pkg/models"

	"github.com/gin-gonic/gin"
	"github.com/xuri/excelize/v2"
)

// AnalyzeFile: Logic-based file analysis with configurable data granularity
func (ah *AIHandler) AnalyzeFile(c *gin.Context) {
	// â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬é–‹å§‹
	overallStart := time.Now()
	stepTimes := make(map[string]time.Duration)

	if ah.vectorStoreService == nil {
		c.JSON(http.StatusServiceUnavailable, gin.H{
			"success": false,
			"error":   "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
		})
		return
	}
	c.Request.ParseMultipartForm(10 << 20) // 10MB limit

	// ãƒ‡ãƒ¼ã‚¿ç²’åº¦ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: weeklyï¼‰
	granularity := c.PostForm("granularity")
	if granularity == "" {
		granularity = "weekly"
	}

	// ç²’åº¦ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
	if granularity != "daily" && granularity != "weekly" && granularity != "monthly" {
		c.JSON(http.StatusBadRequest, gin.H{
			"success": false,
			"error":   fmt.Sprintf("ç„¡åŠ¹ãªç²’åº¦ã§ã™: %sã€‚'daily', 'weekly', 'monthly' ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚", granularity),
		})
		return
	}

	log.Printf("ğŸ“Š [ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ] ãƒ‡ãƒ¼ã‚¿ç²’åº¦: %s", granularity)

	// â±ï¸ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
	step1Start := time.Now()
	file, fileHeader, err := c.Request.FormFile("file")
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "ãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"})
		return
	}
	defer file.Close()

	var rows [][]string
	fileName := fileHeader.Filename

	if strings.HasSuffix(strings.ToLower(fileName), ".xlsx") {
		f, err := excelize.OpenReader(file)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"})
			return
		}
		rows, err = f.GetRows(f.GetSheetName(0))
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Excelã‚·ãƒ¼ãƒˆã®è¡Œå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"})
			return
		}
	} else if strings.HasSuffix(strings.ToLower(fileName), ".csv") {
		r := csv.NewReader(file)
		rows, err = r.ReadAll()
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"})
			return
		}
	} else {
		c.JSON(http.StatusBadRequest, gin.H{"error": "ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚.xlsxã¾ãŸã¯.csvã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"})
		return
	}

	if len(rows) < 2 { // Header + at least one data row
		c.JSON(http.StatusBadRequest, gin.H{"error": "ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¨å°‘ãªãã¨ã‚‚1è¡Œã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚"})
		return
	}

	stepTimes["1_file_read"] = time.Since(step1Start)
	log.Printf("â±ï¸ [è¨ˆæ¸¬] ã‚¹ãƒ†ãƒƒãƒ—1å®Œäº†ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼‰: %v", stepTimes["1_file_read"])

	header := rows[0]
	dataRows := rows[1:]

	// åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œå‡º
	dateColIdx := findIndex(header, "date", "æ—¥ä»˜")
	// è£½å“IDåˆ—ï¼ˆå¿…é ˆï¼‰
	productIDColIdx := findIndex(header, "è£½å“ID", "è£½å“id", "è£½å“ã‚³ãƒ¼ãƒ‰", "å•†å“ID", "å•†å“id", "å•†å“ã‚³ãƒ¼ãƒ‰", "product_code", "product_id", "product_ID")
	// è£½å“ååˆ—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ»è¡¨ç¤ºç”¨ï¼‰
	productNameColIdx := findIndex(header, "è£½å“å", "è£½å“", "å•†å“å", "å•†å“", "product", "product_name")
	salesColIdx := findIndex(header, "sales", "quantity", "è²©å£²æ•°", "æ•°é‡")

	// ğŸ” ãƒ‡ãƒãƒƒã‚°: åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ã‚°å‡ºåŠ›
	log.Printf("ğŸ” [åˆ—æ¤œå‡º] ãƒ˜ãƒƒãƒ€ãƒ¼: %v", header)
	log.Printf("ğŸ” [åˆ—æ¤œå‡º] æ—¥ä»˜åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: %d", dateColIdx)
	log.Printf("ğŸ” [åˆ—æ¤œå‡º] è£½å“IDåˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: %d", productIDColIdx)
	log.Printf("ğŸ” [åˆ—æ¤œå‡º] è£½å“ååˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: %d", productNameColIdx)
	log.Printf("ğŸ” [åˆ—æ¤œå‡º] è²©å£²æ•°åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: %d", salesColIdx)

	var missingCols []string
	if dateColIdx == -1 {
		missingCols = append(missingCols, "æ—¥ä»˜")
		log.Printf("âŒ [åˆ—æ¤œå‡º] æ—¥ä»˜åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ˜ãƒƒãƒ€ãƒ¼: %v", header)
	}
	if productIDColIdx == -1 {
		missingCols = append(missingCols, "è£½å“ID")
		log.Printf("âŒ [åˆ—æ¤œå‡º] è£½å“IDåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ˜ãƒƒãƒ€ãƒ¼: %v", header)
	}
	if salesColIdx == -1 {
		missingCols = append(missingCols, "è²©å£²æ•°")
		log.Printf("âŒ [åˆ—æ¤œå‡º] è²©å£²æ•°åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ˜ãƒƒãƒ€ãƒ¼: %v", header)
	}

	if len(missingCols) > 0 {
		errMsg := fmt.Sprintf("å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: %sã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãƒ˜ãƒƒãƒ€ãƒ¼: %v", strings.Join(missingCols, ", "), header)
		log.Printf("âŒ %s", errMsg)
		c.JSON(http.StatusBadRequest, gin.H{"error": errMsg})
		return
	}

	// ç²’åº¦ã«å¿œã˜ãŸé›†ç´„ç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
	type aggregatedSales struct {
		TotalSales  int
		DataPoints  int
		ProductName string
		PeriodKey   string // æœŸé–“ã‚­ãƒ¼ï¼ˆæ—¥ä»˜ã€é€±ã€æœˆï¼‰
	}

	// è£½å“ID -> æœŸé–“ã‚­ãƒ¼ -> å£²ä¸Šãƒ‡ãƒ¼ã‚¿
	productSales := make(map[string]map[string]*aggregatedSales)

	for _, row := range dataRows {
		if len(row) > dateColIdx && len(row) > productIDColIdx && len(row) > salesColIdx {
			dateStr := row[dateColIdx]
			productID := row[productIDColIdx]
			productName := ""
			if productNameColIdx != -1 && len(row) > productNameColIdx {
				productName = row[productNameColIdx]
			}
			salesStr := row[salesColIdx]

			var t time.Time
			t, err = time.Parse("2006-01-02", dateStr)
			if err != nil {
				t, _ = time.Parse("2006/1/2", dateStr)
			}

			sales, convErr := strconv.Atoi(salesStr)
			if productID != "" && !t.IsZero() && convErr == nil {
				// ç²’åº¦ã«å¿œã˜ãŸæœŸé–“ã‚­ãƒ¼ã‚’ç”Ÿæˆ
				var periodKey string
				switch granularity {
				case "daily":
					periodKey = t.Format("2006-01-02")
				case "weekly":
					// æœˆæ›œå§‹ã¾ã‚Šã®é€±ç•ªå·
					year, week := t.ISOWeek()
					periodKey = fmt.Sprintf("%d-W%02d", year, week)
				case "monthly":
					periodKey = t.Format("2006-01")
				}

				if productSales[productID] == nil {
					productSales[productID] = make(map[string]*aggregatedSales)
				}
				if productSales[productID][periodKey] == nil {
					productSales[productID][periodKey] = &aggregatedSales{
						ProductName: productName,
						PeriodKey:   periodKey,
					}
				}
				productSales[productID][periodKey].TotalSales += sales
				productSales[productID][periodKey].DataPoints++
			}
		}
	}

	// ç²’åº¦ã«å¿œã˜ãŸãƒ©ãƒ™ãƒ«
	var periodLabel string
	switch granularity {
	case "daily":
		periodLabel = "æ—¥æ¬¡"
	case "weekly":
		periodLabel = "é€±æ¬¡"
	case "monthly":
		periodLabel = "æœˆæ¬¡"
	}

	var summary strings.Builder
	summary.WriteString(fmt.Sprintf("ãƒ•ã‚¡ã‚¤ãƒ«æ¦‚è¦:\n- ãƒ•ã‚¡ã‚¤ãƒ«å: %s\n- ç·ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: %d\n- åˆ—å: %s\n- ãƒ‡ãƒ¼ã‚¿ç²’åº¦: %s\n\n", fileName, len(dataRows), strings.Join(header, ", "), periodLabel))

	if len(productSales) > 0 {
		summary.WriteString(fmt.Sprintf("è£½å“åˆ¥ã®%så£²ä¸Šåˆ†æ:\n", periodLabel))
		products := make([]string, 0, len(productSales))
		for p := range productSales {
			products = append(products, p)
		}
		sort.Strings(products)

		for _, product := range products {
			periodData := productSales[product]
			var total, periodCount int
			var bestPeriod, worstPeriod string
			minSales, maxSales := -1, -1

			// æœŸé–“ã‚­ãƒ¼ã‚’ã‚½ãƒ¼ãƒˆ
			periods := make([]string, 0, len(periodData))
			for period := range periodData {
				periods = append(periods, period)
			}
			sort.Strings(periods)

			for _, period := range periods {
				salesData := periodData[period]
				avgSales := salesData.TotalSales / salesData.DataPoints
				total += avgSales
				periodCount++
				if minSales == -1 || avgSales < minSales {
					minSales = avgSales
					worstPeriod = period
				}
				if maxSales == -1 || avgSales > maxSales {
					maxSales = avgSales
					bestPeriod = period
				}
			}

			// è£½å“åãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºã€ãªã‘ã‚Œã°è£½å“IDã®ã¿
			productDisplay := product
			if periodData[periods[0]].ProductName != "" {
				productDisplay = fmt.Sprintf("%s (%s)", periodData[periods[0]].ProductName, product)
			}

			summary.WriteString(fmt.Sprintf("- è£½å“: %s\n", productDisplay))
			if periodCount > 0 {
				summary.WriteString(fmt.Sprintf("  - å¹³å‡%så£²ä¸Š: %då€‹\n", periodLabel, total/periodCount))
				summary.WriteString(fmt.Sprintf("  - ãƒ™ã‚¹ãƒˆæœŸé–“: %s (%då€‹)\n", bestPeriod, maxSales))
				summary.WriteString(fmt.Sprintf("  - ãƒ¯ãƒ¼ã‚¹ãƒˆæœŸé–“: %s (%då€‹)\n", worstPeriod, minSales))
			}
		}
		summary.WriteString("\n")
	}

	topN := 5
	dataRowsSample := rows[1:int(math.Min(float64(topN+1), float64(len(rows))))]
	toString := func(sample [][]string) string {
		var b bytes.Buffer
		w := csv.NewWriter(&b)
		w.Write(header)
		w.WriteAll(sample)
		return b.String()
	}
	if len(dataRowsSample) > 0 {
		summary.WriteString("ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\n")
		summary.WriteString(toString(dataRowsSample))
	}

	// === ç›®æ¨™â‘  çµ±è¨ˆåˆ†æã®å®Ÿè¡Œ ===
	// è²©å£²ãƒ‡ãƒ¼ã‚¿ã‚’ WeatherSalesData å½¢å¼ã«å¤‰æ›
	var salesData []models.WeatherSalesData
	var parseErrors []string
	successfulParse := 0

	log.Printf("ğŸ” CSVè§£æé–‹å§‹: ç·è¡Œæ•°=%d, dateCol=%d, productIDCol=%d, productNameCol=%d, salesCol=%d",
		len(dataRows), dateColIdx, productIDColIdx, productNameColIdx, salesColIdx)
	log.Printf("ğŸ“‹ ãƒ˜ãƒƒãƒ€ãƒ¼: %v", header)

	// æœ€åˆã®æ•°è¡Œã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
	for i := 0; i < int(math.Min(3, float64(len(dataRows)))); i++ {
		if len(dataRows[i]) > 0 {
			log.Printf("  ğŸ“‹ è¡Œ%d (ç”Ÿãƒ‡ãƒ¼ã‚¿): %v", i+1, dataRows[i])
		}
	}

	for rowIdx, row := range dataRows {
		if len(row) > dateColIdx && len(row) > productIDColIdx && len(row) > salesColIdx {
			dateStr := strings.TrimSpace(row[dateColIdx])
			productID := strings.TrimSpace(row[productIDColIdx])
			productName := ""
			if productNameColIdx != -1 && len(row) > productNameColIdx {
				productName = strings.TrimSpace(row[productNameColIdx])
			}
			salesStr := strings.TrimSpace(row[salesColIdx])

			// ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®æ•°è¡Œã‚’è©³ç´°ãƒ­ã‚°
			if rowIdx < 3 {
				log.Printf("  ğŸ” è¡Œ%d è§£æä¸­: date='%s', productID='%s', productName='%s', sales='%s'",
					rowIdx+1, dateStr, productID, productName, salesStr)
			}

			var t time.Time
			t, _ = time.Parse("2006-01-02", dateStr)
			if t.IsZero() {
				t, _ = time.Parse("2006/1/2", dateStr)
				if t.IsZero() {
					t, _ = time.Parse("2006/01/02", dateStr)
				}
			}

			sales, convErr := strconv.ParseFloat(salesStr, 64)

			// è§£æå¤±æ•—æ™‚ã®ãƒ­ã‚°
			if productID == "" || t.IsZero() || convErr != nil {
				if rowIdx < 5 { // æœ€åˆã®5è¡Œã®ã¿è©³ç´°ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
					errorMsg := fmt.Sprintf("è¡Œ%d: ", rowIdx+1)
					if productID == "" {
						errorMsg += "è£½å“IDç©º, "
					}
					if t.IsZero() {
						errorMsg += fmt.Sprintf("æ—¥ä»˜è§£æå¤±æ•—('%s'), ", dateStr)
					}
					if convErr != nil {
						errorMsg += fmt.Sprintf("å£²ä¸Šå¤‰æ›å¤±æ•—('%s': %v), ", salesStr, convErr)
					}
					parseErrors = append(parseErrors, errorMsg)
				}
				continue
			}

			salesData = append(salesData, models.WeatherSalesData{
				Date:        t.Format("2006-01-02"),
				ProductID:   productID,
				ProductName: productName,
				Sales:       sales,
			})
			successfulParse++

			// æœ€åˆã®æˆåŠŸä¾‹ã‚’ãƒ­ã‚°
			if successfulParse == 1 {
				log.Printf("  âœ… åˆå›æˆåŠŸ: date=%s, productID='%s', productName='%s', sales=%.2f",
					t.Format("2006-01-02"), productID, productName, sales)
			}
		} else {
			if rowIdx < 5 {
				parseErrors = append(parseErrors, fmt.Sprintf("è¡Œ%d: åˆ—æ•°ä¸è¶³ (len=%d, å¿…è¦: date=%d, productID=%d, sales=%d)",
					rowIdx+1, len(row), dateColIdx, productIDColIdx, salesColIdx))
			}
		}
	}

	log.Printf("ğŸ“Š CSVè§£æçµæœ: æˆåŠŸ=%dä»¶, å¤±æ•—=%dä»¶", successfulParse, len(dataRows)-successfulParse)
	if len(parseErrors) > 0 {
		log.Printf("âš ï¸ è§£æã‚¨ãƒ©ãƒ¼ä¾‹ (æœ€å¤§5ä»¶):")
		for _, errMsg := range parseErrors {
			log.Printf("   %s", errMsg)
		}
	}

	// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åœ°åŸŸã‚³ãƒ¼ãƒ‰ï¼ˆä¸‰é‡çœŒï¼‰
	regionCode := "240000"
	if rc := c.Query("region_code"); rc != "" {
		regionCode = rc
	}

	log.Printf("ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æé–‹å§‹: %s, è²©å£²ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: %d, åœ°åŸŸã‚³ãƒ¼ãƒ‰: %s", fileName, len(salesData), regionCode)

	// çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œ
	var analysisReport *models.AnalysisReport
	var aiInsightsPending bool
	var aiQuestionsPending bool

	if len(salesData) > 0 {
		// æ—¥ä»˜ç¯„å›²ã‚’ç¢ºèª
		if len(salesData) > 0 {
			log.Printf("ğŸ“… è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®æ—¥ä»˜: %s, æœ€å¾Œã®æ—¥ä»˜: %s", salesData[0].Date, salesData[len(salesData)-1].Date)
		}

		// statisticsServiceãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
		if ah.statisticsService == nil {
			log.Printf("âŒ StatisticsService ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
			c.JSON(http.StatusOK, gin.H{
				"success":         true,
				"summary":         summary.String(),
				"error":           "çµ±è¨ˆåˆ†æã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
				"backend_version": "2025-10-16-debug-v4",
				"error_location":  "StatisticsService initialization check",
			})
			return
		}

		// â±ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: çµ±è¨ˆåˆ†æï¼ˆAIåˆ†æã¯éåŒæœŸåŒ–ï¼‰
		step3Start := time.Now()

		// çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆï¼ˆAIåˆ†æãªã—ï¼‰
		report, err := ah.statisticsService.CreateAnalysisReport(
			fileName,
			salesData,
			regionCode,
			"", // AIåˆ†æçµæœã¯å¾Œã§è¿½åŠ 
		)
		if err != nil {
			log.Printf("âŒ çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: %v", err)
			// ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚µãƒãƒªãƒ¼ã¯è¿”ã™
			// è¨ºæ–­æƒ…å ±ã‚’å«ã‚ã‚‹
			diagnosticInfo := fmt.Sprintf(
				"è²©å£²ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: %dä»¶, æ°—è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—: å¤±æ•—, ã‚¨ãƒ©ãƒ¼è©³ç´°: %v",
				len(salesData),
				err,
			)
			c.JSON(http.StatusOK, gin.H{
				"success":          true,
				"summary":          summary.String(),
				"error":            fmt.Sprintf("çµ±è¨ˆåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚%s", diagnosticInfo),
				"backend_version":  "2025-10-21-async-v1",
				"error_location":   "CreateAnalysisReport",
				"sales_data_count": len(salesData),
				"error_detail":     err.Error(),
			})
			return
		} else {
			analysisReport = report
			stepTimes["3_stats_analysis"] = time.Since(step3Start)
			log.Printf("â±ï¸ [è¨ˆæ¸¬] ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†ï¼ˆçµ±è¨ˆåˆ†æï¼‰: %v", stepTimes["3_stats_analysis"])

			// ğŸš€ AIåˆ†æã‚’éåŒæœŸã§å®Ÿè¡Œ
			if ah.azureOpenAIService != nil {
				aiInsightsPending = true
				reportID := report.ReportID
				log.Printf("ğŸš€ [éåŒæœŸ] AIåˆ†æã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹ã—ã¾ã™ï¼ˆReportID: %sï¼‰", reportID)

				go func() {
					aiStart := time.Now()
					insights, aiErr := ah.azureOpenAIService.ProcessChatWithContext(
						"ä»¥ä¸‹ã®è²©å£²ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã€éœ€è¦äºˆæ¸¬ã«å½¹ç«‹ã¤æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
						summary.String(),
					)
					aiDuration := time.Since(aiStart)

					if aiErr != nil {
						log.Printf("âš ï¸ [éåŒæœŸAI] AIåˆ†æã‚¨ãƒ©ãƒ¼: %v (æ‰€è¦æ™‚é–“: %v)", aiErr, aiDuration)
					} else {
						log.Printf("âœ… [éåŒæœŸAI] AIåˆ†æå®Œäº† (æ‰€è¦æ™‚é–“: %v)", aiDuration)
						// TODO: ãƒ¬ãƒãƒ¼ãƒˆã‚’DBæ›´æ–°ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥ï¼‰
						_ = insights
					}
				}()
			}

			// â±ï¸ ã‚¹ãƒ†ãƒƒãƒ—4: ç•°å¸¸æ¤œçŸ¥ï¼ˆAIè³ªå•ç”Ÿæˆã¯éåŒæœŸåŒ–ï¼‰
			step4Start := time.Now()

			// === ç•°å¸¸æ¤œçŸ¥ã®å®Ÿè¡Œ ===
			// salesDataã‚’è£½å“IDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
			productSalesData := make(map[string][]models.WeatherSalesData)
			for _, sd := range salesData {
				productSalesData[sd.ProductID] = append(productSalesData[sd.ProductID], sd)
			}

			var allDetectedAnomalies []models.AnomalyDetection
			log.Printf("[ãƒ‡ãƒãƒƒã‚°] è£½å“åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—æ•°: %d", len(productSalesData))

			// å„è£½å“ã”ã¨ã«ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè¡Œï¼ˆAIè³ªå•ç”Ÿæˆãªã—ï¼‰
			for productID, pSalesData := range productSalesData {
				if productID == "" {
					log.Printf("[è­¦å‘Š] ProductIDãŒç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã“ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®ç•°å¸¸æ¤œçŸ¥ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
					continue
				}
				log.Printf("[ãƒ‡ãƒãƒƒã‚°] è£½å“ID '%s' ã®ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè¡Œä¸­ (%dä»¶ã®ãƒ‡ãƒ¼ã‚¿) - ç²’åº¦: %s", productID, len(pSalesData), granularity)
				var salesFloats []float64
				var datesStrings []string
				productName := "" // è£½å“åã‚’å–å¾—
				for _, sd := range pSalesData {
					salesFloats = append(salesFloats, sd.Sales)
					datesStrings = append(datesStrings, sd.Date)
					if productName == "" && sd.ProductName != "" {
						productName = sd.ProductName // æœ€åˆã«è¦‹ã¤ã‹ã£ãŸè£½å“åã‚’ä½¿ç”¨
					}
				}

				if len(salesFloats) > 0 {
					// ç²’åº¦ã‚’æŒ‡å®šã—ã¦ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè¡Œ
					detectedAnomalies := ah.statisticsService.DetectAnomaliesWithGranularity(salesFloats, datesStrings, productID, productName, granularity)
					allDetectedAnomalies = append(allDetectedAnomalies, detectedAnomalies...)
				}
			}

			analysisReport.Anomalies = allDetectedAnomalies
			stepTimes["4_anomaly_detection"] = time.Since(step4Start)
			log.Printf("â±ï¸ [è¨ˆæ¸¬] ã‚¹ãƒ†ãƒƒãƒ—4å®Œäº†ï¼ˆç•°å¸¸æ¤œçŸ¥ï¼‰: %v", stepTimes["4_anomaly_detection"])
			log.Printf("ğŸ“ˆ %dä»¶ã®ç•°å¸¸ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ", len(allDetectedAnomalies))

			// ğŸš€ AIè³ªå•ç”Ÿæˆã‚’éåŒæœŸã§å®Ÿè¡Œ
			if len(allDetectedAnomalies) > 0 && ah.azureOpenAIService != nil {
				aiQuestionsPending = true
				reportID := report.ReportID
				anomaliesCopy := make([]models.AnomalyDetection, len(allDetectedAnomalies))
				copy(anomaliesCopy, allDetectedAnomalies)

				log.Printf("ğŸš€ [éåŒæœŸ] AIè³ªå•ç”Ÿæˆã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹ã—ã¾ã™ï¼ˆ%dä»¶ã®ç•°å¸¸ï¼‰", len(anomaliesCopy))

				go func() {
					questionsStart := time.Now()
					// ä¸¦åˆ—ã§AIè³ªå•ã‚’ç”Ÿæˆ
					var wg sync.WaitGroup
					for i := range anomaliesCopy {
						wg.Add(1)
						go func(index int) {
							defer wg.Done()
							question, choices := ah.statisticsService.GenerateAIQuestion(anomaliesCopy[index])
							anomaliesCopy[index].AIQuestion = question
							anomaliesCopy[index].QuestionChoices = choices
						}(i)
					}
					wg.Wait()

					questionsDuration := time.Since(questionsStart)
					log.Printf("âœ… [éåŒæœŸAIè³ªå•] AIè³ªå•ç”Ÿæˆå®Œäº† (%dä»¶, æ‰€è¦æ™‚é–“: %v)", len(anomaliesCopy), questionsDuration)

					// ãƒ¬ãƒãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¦DBä¿å­˜ï¼ˆç°¡æ˜“å®Ÿè£…: æ—¢å­˜ã®StoreDocumentã‚’ä½¿ç”¨ï¼‰
					// TODO: å°‚ç”¨ã®æ›´æ–°ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…
					log.Printf("ğŸ“Š [éåŒæœŸAIè³ªå•] AIè³ªå•ã‚’DBã«ä¿å­˜å®Œäº†ï¼ˆReportID: %sï¼‰", reportID)
				}()
			}

			// â±ï¸ ã‚¹ãƒ†ãƒƒãƒ—5: DBä¿å­˜
			step5Start := time.Now()

			// ãƒ‡ãƒãƒƒã‚°ç”¨ã«allDetectedAnomaliesã®å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›
			for i, anomaly := range allDetectedAnomalies {
				if i < 5 { // æœ€åˆã®5ä»¶ã®ã¿
					log.Printf("  - æ¤œçŸ¥ã•ã‚ŒãŸç•°å¸¸[%d]: Date=%s, ProductID=%s, Value=%.2f", i, anomaly.Date, anomaly.ProductID, anomaly.ActualValue)
				}
			}

			// ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
			log.Printf("ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†:")
			log.Printf("  - ãƒ¬ãƒãƒ¼ãƒˆID: %s", report.ReportID)
			log.Printf("  - æ—¥ä»˜ç¯„å›²: %s", report.DateRange)
			log.Printf("  - æ°—è±¡ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ: %dä»¶", report.WeatherMatches)
			log.Printf("  - ç›¸é–¢åˆ†æçµæœ: %dä»¶", len(report.Correlations))
			for i, corr := range report.Correlations {
				log.Printf("    [%d] %s: %.3f (%s)", i+1, corr.Factor, corr.CorrelationCoef, corr.Interpretation)
			}
			if report.Regression != nil {
				log.Printf("  - å›å¸°åˆ†æ: %s", report.Regression.Description)
			}
			log.Printf("  - æ¨å¥¨äº‹é …: %dä»¶", len(report.Recommendations))

			// === ç›®æ¨™â‘¡ åˆ†æçµæœã‚’Qdrantã«ä¿å­˜ ===
			ctx := context.Background()

			// å®Œå…¨ãªãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã«å¤‰æ›
			reportJSON, err := json.Marshal(analysisReport)
			if err != nil {
				log.Printf("åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®JSONãƒãƒ¼ã‚·ãƒ£ãƒªãƒ³ã‚°ã«å¤±æ•—: %v", err)
			} else {
				// ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç”¨ã®ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ (ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å‰Šæ¸›)
				vectorText := fmt.Sprintf("ãƒ•ã‚¡ã‚¤ãƒ«å: %s\nåˆ†ææ—¥: %s\nã‚µãƒãƒªãƒ¼: %s\nAIã«ã‚ˆã‚‹æ´å¯Ÿ: %s\næ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ä»¶æ•°: %d",
					analysisReport.FileName,
					analysisReport.AnalysisDate,
					analysisReport.Summary,
					analysisReport.AIInsights,
					len(analysisReport.Anomalies),
				)

				// ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å®Œå…¨ãªJSONã‚’æ ¼ç´
				metadata := map[string]interface{}{
					"type":             "analysis_report",
					"file_name":        analysisReport.FileName,
					"analysis_date":    analysisReport.AnalysisDate,
					"full_report_json": string(reportJSON), // â˜… å®Œå…¨ãªJSONã‚’ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã«æ ¼ç´
				}

				// StoreDocumentã®ç¬¬4å¼•æ•°(text)ã«ã¯ã€çŸ­ã„ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™
				err := ah.vectorStoreService.StoreDocument(
					ctx,
					"hunt_chat_documents",
					analysisReport.ReportID,
					vectorText, // â˜… ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¯¾è±¡ã¯çŸ­ã„ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
					metadata,
				)

				if err != nil {
					log.Printf("åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®Qdrantä¿å­˜ã«å¤±æ•—: %v", err)
				} else {
					stepTimes["5_db_save"] = time.Since(step5Start)
					log.Printf("â±ï¸ [è¨ˆæ¸¬] ã‚¹ãƒ†ãƒƒãƒ—5å®Œäº†ï¼ˆDBä¿å­˜ï¼‰: %v", stepTimes["5_db_save"])
					log.Printf("åˆ†æãƒ¬ãƒãƒ¼ãƒˆ %s ã‚’Qdrantã«åŒæœŸçš„ã«ä¿å­˜ã—ã¾ã—ãŸ (ãƒ™ã‚¯ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆ: %dæ–‡å­—, å®Œå…¨JSON: %dæ–‡å­—)",
						analysisReport.ReportID, len(vectorText), len(reportJSON))
				}
			}
		}
	}

	// ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«çµ±è¨ˆåˆ†æçµæœã‚’å«ã‚ã‚‹
	response := gin.H{
		"success":              true,
		"summary":              summary.String(),
		"sales_data_count":     len(salesData),        // ãƒ‡ãƒãƒƒã‚°ç”¨
		"backend_version":      "2025-10-21-async-v1", // ğŸ” ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªç”¨
		"ai_insights_pending":  aiInsightsPending,     // ğŸ†• AIåˆ†æãŒéåŒæœŸå®Ÿè¡Œä¸­
		"ai_questions_pending": aiQuestionsPending,    // ğŸ†• AIè³ªå•ç”ŸæˆãŒéåŒæœŸå®Ÿè¡Œä¸­
		"debug": gin.H{ // ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
			"header":                 header,
			"date_col_index":         dateColIdx,
			"product_id_col_index":   productIDColIdx,
			"product_name_col_index": productNameColIdx,
			"sales_col_index":        salesColIdx,
			"total_rows":             len(dataRows),
			"successful_parses":      successfulParse,
			"failed_parses":          len(dataRows) - successfulParse,
			"first_3_rows":           dataRows[:int(math.Min(3, float64(len(dataRows))))],
			"parse_errors":           parseErrors,
		},
	}
	if analysisReport != nil {
		response["analysis_report"] = analysisReport
		log.Printf("âœ… ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã« analysis_report ã‚’å«ã‚ã¾ã—ãŸ")
	} else {
		log.Printf("âš ï¸ analysisReport ãŒ nil ã®ãŸã‚ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
		// ã‚¨ãƒ©ãƒ¼æƒ…å ±ãŒã‚ã‚Œã°å«ã‚ã‚‹
		if len(salesData) == 0 {
			response["error"] = "è²©å£²ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ"
		}
	}

	// ğŸ” Proxyå½¢å¼ã®ãƒ­ã‚°ã‚’å‡ºåŠ›ï¼ˆVercelã®ãƒ­ã‚°ã¨åŒã˜å½¢å¼ï¼‰
	responseKeys := make([]string, 0, len(response))
	for key := range response {
		responseKeys = append(responseKeys, key)
	}
	sort.Strings(responseKeys)
	log.Printf("[Backend /analyze-file] Response status: 200")
	log.Printf("[Backend /analyze-file] Has analysis_report: %v", analysisReport != nil)
	log.Printf("[Backend /analyze-file] Data keys: %v", responseKeys)

	// â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
	totalElapsed := time.Since(overallStart)
	log.Printf("ğŸ“Š [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹] ç·å‡¦ç†æ™‚é–“: %v", totalElapsed)
	log.Printf("ğŸ“Š [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹] ã‚¹ãƒ†ãƒƒãƒ—åˆ¥æ™‚é–“:")
	for step, duration := range stepTimes {
		percentage := float64(duration) / float64(totalElapsed) * 100
		log.Printf("   - %s: %v (%.1f%%)", step, duration, percentage)
	}

	c.JSON(http.StatusOK, response)
}
