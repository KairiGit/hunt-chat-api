package services

import (
	"fmt"
	"log"
	"math"
	"sort"
	"strings"
	"time"

	"hunt-chat-api/pkg/models"

	"github.com/google/uuid"
)

// StatisticsService çµ±è¨ˆåˆ†æã‚µãƒ¼ãƒ“ã‚¹
type StatisticsService struct {
	weatherService     *WeatherService
	economicService    *EconomicService
	azureOpenAIService *AzureOpenAIService
}

// NewStatisticsService æ–°ã—ã„çµ±è¨ˆåˆ†æã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
func NewStatisticsService(weatherService *WeatherService, economicService *EconomicService, azureOpenAIService *AzureOpenAIService) *StatisticsService {
	return &StatisticsService{
		weatherService:     weatherService,
		economicService:    economicService,
		azureOpenAIService: azureOpenAIService,
	}
}

// CalculateCorrelation 2ã¤ã®ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
func (s *StatisticsService) CalculateCorrelation(x, y []float64) (float64, error) {
	if len(x) != len(y) || len(x) == 0 {
		return 0, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ãªã„ã‹ã€ç©ºã§ã™")
	}

	n := float64(len(x))
	var sumX, sumY, sumXY, sumX2, sumY2 float64

	for i := 0; i < len(x); i++ {
		sumX += x[i]
		sumY += y[i]
		sumXY += x[i] * y[i]
		sumX2 += x[i] * x[i]
		sumY2 += y[i] * y[i]
	}

	numerator := n*sumXY - sumX*sumY
	denominator := math.Sqrt((n*sumX2 - sumX*sumX) * (n*sumY2 - sumY*sumY))

	if denominator == 0 {
		return 0, fmt.Errorf("åˆ†æ¯ãŒ0ã«ãªã‚Šã¾ã—ãŸï¼ˆæ¨™æº–åå·®ãŒ0ï¼‰")
	}

	return numerator / denominator, nil
}

// CalculateLaggedCorrelations x(t) vs y(t+lag) for lag in [-maxLagDays, +maxLagDays].
// Returns a sorted list by absolute correlation desc.
func (s *StatisticsService) CalculateLaggedCorrelations(xDates []string, xVals []float64, yDates []string, yVals []float64, maxLagDays int) ([]models.CorrelationResult, error) {
	if len(xDates) != len(xVals) || len(yDates) != len(yVals) {
		return nil, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“")
	}
	if len(xVals) < 5 || len(yVals) < 5 {
		return nil, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½5ç‚¹ï¼‰")
	}
	// Build map for fast lookup
	xMap := make(map[string]float64, len(xDates))
	for i, d := range xDates {
		xMap[d] = xVals[i]
	}
	yMap := make(map[string]float64, len(yDates))
	for i, d := range yDates {
		yMap[d] = yVals[i]
	}

	// Helper to shift dates by lag days
	shift := func(date string, lag int) (string, bool) {
		t, err := time.Parse("2006-01-02", date)
		if err != nil {
			return "", false
		}
		return t.AddDate(0, 0, lag).Format("2006-01-02"), true
	}

	var results []models.CorrelationResult
	for lag := -maxLagDays; lag <= maxLagDays; lag++ {
		var xs, ys []float64
		// align on x dates
		for _, d := range xDates {
			if d == "" {
				continue
			}
			shifted, ok := shift(d, lag)
			if !ok {
				continue
			}
			xv, okx := xMap[d]
			yv, oky := yMap[shifted]
			if okx && oky {
				xs = append(xs, xv)
				ys = append(ys, yv)
			}
		}
		if len(xs) >= 5 && len(xs) == len(ys) {
			r, err := s.CalculateCorrelation(xs, ys)
			if err == nil {
				p := s.CalculatePValue(r, len(xs))
				label := "lag=0"
				if lag > 0 {
					label = fmt.Sprintf("yãŒxã«å¯¾ã—ã¦+%dæ—¥é…ã‚Œ", lag)
				}
				if lag < 0 {
					label = fmt.Sprintf("yãŒxã«å¯¾ã—ã¦%dæ—¥å…ˆè¡Œ", -lag)
				}
				results = append(results, models.CorrelationResult{
					Factor:          label,
					CorrelationCoef: r,
					PValue:          p,
					SampleSize:      len(xs),
					Interpretation:  s.InterpretCorrelation(r, p),
				})
			}
		}
	}
	sort.Slice(results, func(i, j int) bool {
		return math.Abs(results[i].CorrelationCoef) > math.Abs(results[j].CorrelationCoef)
	})
	return results, nil
}

// CalculatePValue ç›¸é–¢ä¿‚æ•°ã®på€¤ã‚’è¿‘ä¼¼è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
func (s *StatisticsService) CalculatePValue(r float64, n int) float64 {
	if n < 3 {
		return 1.0
	}
	t := r * math.Sqrt(float64(n-2)) / math.Sqrt(1-r*r)
	df := float64(n - 2)
	// Use Student's t CDF for two-tailed p-value
	p := 2 * (1 - studentTCDF(math.Abs(t), df))
	if p < 0 {
		p = 0
	}
	if p > 1 {
		p = 1
	}
	return p
}

// studentTCDF computes the CDF of Student's t at x with df degrees of freedom.
// Uses regularized incomplete beta relation for accuracy.
func studentTCDF(x, df float64) float64 {
	// For x=0, CDF=0.5
	if x == 0 {
		return 0.5
	}
	// Relation: CDF = 0.5 + x * Gamma((v+1)/2) * 2F1(...) / (sqrt(v*pi) * Gamma(v/2))
	// We'll use the incomplete beta representation:
	// For t>0: CDF = 1 - 0.5*I_{v/(v+t^2)}(v/2, 1/2)
	// For t<0: CDF = 0.5*I_{v/(v+t^2)}(v/2, 1/2)
	v := df
	t2 := x * x
	z := v / (v + t2)
	ib := regularizedIncompleteBeta(0.5*v, 0.5, z)
	if x > 0 {
		return 1 - 0.5*ib
	}
	return 0.5 * ib
}

// regularizedIncompleteBeta returns I_x(a,b).
// We implement a simple continued fraction via Lentz's algorithm for the incomplete beta function ratio.
func regularizedIncompleteBeta(a, b, x float64) float64 {
	if x <= 0 {
		return 0
	}
	if x >= 1 {
		return 1
	}
	// Use symmetry to improve convergence
	bt := math.Exp(lgamma(a+b) - lgamma(a) - lgamma(b) + a*math.Log(x) + b*math.Log(1-x))
	var ib float64
	if x < (a+1)/(a+b+2) {
		ib = bt * betacf(a, b, x) / a
	} else {
		ib = 1 - bt*betacf(b, a, 1-x)/b
	}
	return ib
}

// betacf computes the continued fraction for incomplete beta using Lentz's algorithm.
func betacf(a, b, x float64) float64 {
	const maxIter = 200
	const eps = 3e-7
	const fpmin = 1e-30
	am := 1.0
	bm := 1.0
	az := 1.0
	qab := a + b
	qap := a + 1
	qam := a - 1
	bz := 1 - qab*x/qap
	var em, tem, d, ap, app, aold float64
	for m := 1; m <= maxIter; m++ {
		em = float64(m)
		tem = em + em
		d = em * (b - em) * x / ((qam + tem) * (a + tem))
		ap = az + d*am
		bp := bz + d*bm
		d = -(a + em) * (qab + em) * x / ((a + tem) * (qap + tem))
		app = ap + d*az
		bpp := bp + d*bz
		am = ap / bpp
		bm = bp / bpp
		az = app / bpp
		bz = 1.0
		aold = az
		if math.Abs((az-aold)/az) < eps {
			return az
		}
		if math.Abs(bpp) < fpmin {
			bpp = fpmin
		}
	}
	return az
}

// lgamma wrapper for math.Lgamma returning sign-less log gamma
func lgamma(x float64) float64 {
	l, _ := math.Lgamma(x)
	return l
}

// AdjustPValuesBH applies Benjamini-Hochberg FDR correction to a slice of p-values.
func (s *StatisticsService) AdjustPValuesBH(pvals []float64) []float64 {
	n := len(pvals)
	type kv struct {
		p float64
		i int
	}
	arr := make([]kv, n)
	for i, p := range pvals {
		arr[i] = kv{p: p, i: i}
	}
	sort.Slice(arr, func(i, j int) bool { return arr[i].p < arr[j].p })
	adj := make([]float64, n)
	var prev float64 = 1.0
	for i := n - 1; i >= 0; i-- {
		rank := float64(i + 1)
		val := arr[i].p * float64(n) / rank
		if val > prev {
			val = prev
		}
		if val > 1 {
			val = 1
		}
		adj[i] = val
		prev = val
	}
	// restore original order
	out := make([]float64, n)
	for idx, a := range adj {
		out[arr[idx].i] = a
	}
	return out
}

// FirstDifference transforms series to first differences to remove trend.
func (s *StatisticsService) FirstDifference(vals []float64) []float64 {
	if len(vals) < 2 {
		return vals
	}
	out := make([]float64, 0, len(vals)-1)
	for i := 1; i < len(vals); i++ {
		out = append(out, vals[i]-vals[i-1])
	}
	return out
}

// Detrend by subtracting linear trend.
func (s *StatisticsService) Detrend(vals []float64) []float64 {
	n := len(vals)
	if n < 2 {
		return vals
	}
	// simple linear regression on index
	xs := make([]float64, n)
	for i := 0; i < n; i++ {
		xs[i] = float64(i + 1)
	}
	reg, err := s.PerformLinearRegression(xs, vals)
	if err != nil {
		return vals
	}
	out := make([]float64, n)
	for i := 0; i < n; i++ {
		out[i] = vals[i] - (reg.Slope*xs[i] + reg.Intercept)
	}
	return out
}

// CalculateLaggedCorrelationsWindowed runs lag scan over sliding windows across time.
// windows of size windowDays, step stepDays, returns best lag per window including p and BH-corrected p over lags.
func (s *StatisticsService) CalculateLaggedCorrelationsWindowed(xDates []string, xVals []float64, yDates []string, yVals []float64, maxLagDays int, windowDays int, stepDays int) ([]map[string]interface{}, error) {
	if len(xDates) != len(xVals) || len(yDates) != len(yVals) {
		return nil, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“")
	}
	if windowDays < 7 {
		return nil, fmt.Errorf("windowDaysã¯7ä»¥ä¸Šã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
	}
	// map for lookup
	xMap := map[string]float64{}
	for i, d := range xDates {
		xMap[d] = xVals[i]
	}
	yMap := map[string]float64{}
	for i, d := range yDates {
		yMap[d] = yVals[i]
	}

	// window loop over x timeline
	var results []map[string]interface{}
	// convert xDates to times
	var times []time.Time
	for _, d := range xDates {
		if t, err := time.Parse("2006-01-02", d); err == nil {
			times = append(times, t)
		}
	}
	if len(times) == 0 {
		return nil, fmt.Errorf("xDatesã®å½¢å¼ãŒä¸æ­£ã§ã™")
	}
	// ensure sorted
	sort.Slice(times, func(i, j int) bool { return times[i].Before(times[j]) })
	startIdx := 0
	for {
		winStart := times[startIdx]
		winEnd := winStart.AddDate(0, 0, windowDays-1)
		// collect window dates
		var wxDates []string
		for _, t := range times {
			if t.Before(winStart) || t.After(winEnd) {
				continue
			}
			wxDates = append(wxDates, t.Format("2006-01-02"))
		}
		if len(wxDates) < 5 {
			break
		}
		// sweep lags
		type lr struct {
			lag int
			r   float64
			p   float64
			n   int
		}
		var all []lr
		for lag := -maxLagDays; lag <= maxLagDays; lag++ {
			var xs, ys []float64
			for _, d := range wxDates {
				t, _ := time.Parse("2006-01-02", d)
				sd := t.AddDate(0, 0, lag).Format("2006-01-02")
				xv, okx := xMap[d]
				yv, oky := yMap[sd]
				if okx && oky {
					xs = append(xs, xv)
					ys = append(ys, yv)
				}
			}
			if len(xs) >= 5 && len(xs) == len(ys) {
				r, err := s.CalculateCorrelation(xs, ys)
				if err == nil {
					p := s.CalculatePValue(r, len(xs))
					all = append(all, lr{lag: lag, r: r, p: p, n: len(xs)})
				}
			}
		}
		if len(all) > 0 {
			// BH correction across lags in the window
			pvals := make([]float64, len(all))
			for i, a := range all {
				pvals[i] = a.p
			}
			padj := s.AdjustPValuesBH(pvals)
			// pick best by |r|
			bestIdx := 0
			for i := 1; i < len(all); i++ {
				if math.Abs(all[i].r) > math.Abs(all[bestIdx].r) {
					bestIdx = i
				}
			}
			res := map[string]interface{}{
				"window_start": winStart.Format("2006-01-02"),
				"window_end":   winEnd.Format("2006-01-02"),
				"best_lag":     all[bestIdx].lag,
				"r":            all[bestIdx].r,
				"p":            all[bestIdx].p,
				"p_adj":        padj[bestIdx],
				"n":            all[bestIdx].n,
			}
			results = append(results, res)
		}
		// advance window
		winNext := winStart.AddDate(0, 0, stepDays)
		// find index closest to winNext
		nextIdx := -1
		for i, t := range times {
			if !t.Before(winNext) {
				nextIdx = i
				break
			}
		}
		if nextIdx == -1 || nextIdx == startIdx {
			break
		}
		startIdx = nextIdx
	}
	return results, nil
}

// InterpretCorrelation ç›¸é–¢ä¿‚æ•°ã‚’äººé–“ãŒèª­ã‚ã‚‹å½¢ã§è§£é‡ˆ
func (s *StatisticsService) InterpretCorrelation(r float64, pValue float64) string {
	absR := math.Abs(r)
	strength := ""

	if absR >= 0.7 {
		strength = "å¼·ã„"
	} else if absR >= 0.4 {
		strength = "ä¸­ç¨‹åº¦ã®"
	} else if absR >= 0.2 {
		strength = "å¼±ã„"
	} else {
		strength = "ã»ã¼ç„¡ã„"
	}

	direction := "æ­£ã®"
	if r < 0 {
		direction = "è² ã®"
	}

	significance := ""
	if pValue < 0.05 {
		significance = "ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰"
	} else {
		significance = "ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ã§ã¯ãªã„ï¼‰"
	}

	return fmt.Sprintf("%s%sç›¸é–¢ %s", strength, direction, significance)
}

// calculateMean å¹³å‡å€¤ã‚’è¨ˆç®—
func (s *StatisticsService) calculateMean(values []float64) float64 {
	if len(values) == 0 {
		return 0
	}
	sum := 0.0
	for _, v := range values {
		sum += v
	}
	return sum / float64(len(values))
}

// calculateStandardDeviation æ¨™æº–åå·®ã‚’è¨ˆç®—
func (s *StatisticsService) calculateStandardDeviation(values []float64) float64 {
	if len(values) == 0 {
		return 0
	}
	mean := s.calculateMean(values)
	sumSquaredDiff := 0.0
	for _, v := range values {
		diff := v - mean
		sumSquaredDiff += diff * diff
	}
	variance := sumSquaredDiff / float64(len(values))
	return math.Sqrt(variance)
}

// AnalyzeSalesWeatherCorrelation è²©å£²ãƒ‡ãƒ¼ã‚¿ã¨æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’åˆ†æï¼ˆé…ã‚Œç›¸é–¢ã‚’å«ã‚€ï¼‰
func (s *StatisticsService) AnalyzeSalesWeatherCorrelation(
	salesData []models.WeatherSalesData,
	regionCode string,
) ([]models.CorrelationResult, error) {

	if len(salesData) == 0 {
		return nil, fmt.Errorf("è²©å£²ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
	}

	// è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ç¯„å›²ã‚’ç‰¹å®š
	var startDate, endDate time.Time
	for i, data := range salesData {
		t, err := time.Parse("2006-01-02", data.Date)
		if err != nil {
			continue
		}
		if i == 0 || t.Before(startDate) {
			startDate = t
		}
		if i == 0 || t.After(endDate) {
			endDate = t
		}
	}

	// æ—¥ä»˜ç¯„å›²ãŒç‰¹å®šã§ããªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆéå»90æ—¥ï¼‰
	if startDate.IsZero() || endDate.IsZero() {
		endDate = time.Now()
		startDate = endDate.AddDate(0, 0, -90)
	}

	// æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆè²©å£²ãƒ‡ãƒ¼ã‚¿ã®æœŸé–“ã«åˆã‚ã›ã‚‹ï¼‰
	weatherData, err := s.weatherService.GetHistoricalWeatherData(regionCode, startDate, endDate)
	if err != nil {
		return nil, fmt.Errorf("æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—: %w", err)
	}

	if len(weatherData) == 0 {
		log.Printf("âš ï¸ æ°—è±¡ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
		return []models.CorrelationResult{}, nil
	}

	// è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã¨å€¤ã‚’æŠ½å‡º
	var salesDates []string
	var salesValues []float64
	for _, sale := range salesData {
		salesDates = append(salesDates, sale.Date)
		salesValues = append(salesValues, sale.Sales)
	}

	// æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã¨å€¤ã‚’æŠ½å‡º
	var weatherDates []string
	var tempValues []float64
	var humValues []float64
	for _, w := range weatherData {
		weatherDates = append(weatherDates, w.Date)
		tempValues = append(tempValues, w.Temperature)
		humValues = append(humValues, w.Humidity)
	}

	if len(salesValues) < 5 {
		return nil, fmt.Errorf("è²©å£²ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ï¼ˆæœ€ä½5ä»¶å¿…è¦ï¼‰")
	}

	// é…ã‚Œç›¸é–¢ã®æœ€å¤§æ—¥æ•°ï¼ˆæ°—è±¡ãƒ‡ãƒ¼ã‚¿ã¯çŸ­æœŸçš„ãªå½±éŸ¿ãŒå¤šã„ãŸã‚çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã‚ˆã‚ŠçŸ­ãè¨­å®šï¼‰
	maxLagDays := 14 // æœ€å¤§14æ—¥ã®é…ã‚Œç›¸é–¢

	var allResults []models.CorrelationResult

	// æ°—æ¸©ã¨ã®é…ã‚Œç›¸é–¢ã‚’è¨ˆç®—
	tempLaggedCorrs, err := s.CalculateLaggedCorrelations(salesDates, salesValues, weatherDates, tempValues, maxLagDays)
	if err != nil {
		log.Printf("âš ï¸ æ°—æ¸©ã®é…ã‚Œç›¸é–¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: %v", err)
	} else {
		// Factoråã« "temperature_" ã‚’è¿½åŠ 
		for i := range tempLaggedCorrs {
			tempLaggedCorrs[i].Factor = fmt.Sprintf("temperature_%s", tempLaggedCorrs[i].Factor)
		}
		// çµ±è¨ˆçš„ã«æœ‰æ„ãªçµæœã®ã¿ã‚’è¿½åŠ 
		for _, corr := range tempLaggedCorrs {
			if corr.PValue < 0.05 || math.Abs(corr.CorrelationCoef) >= 0.3 {
				allResults = append(allResults, corr)
			}
		}
		log.Printf("âœ… æ°—æ¸©ã®é…ã‚Œç›¸é–¢åˆ†æå®Œäº†: %dä»¶ã®æœ‰æ„ãªç›¸é–¢ã‚’æ¤œå‡º", len(tempLaggedCorrs))
	}

	// æ¹¿åº¦ã¨ã®é…ã‚Œç›¸é–¢ã‚’è¨ˆç®—
	humLaggedCorrs, err := s.CalculateLaggedCorrelations(salesDates, salesValues, weatherDates, humValues, maxLagDays)
	if err != nil {
		log.Printf("âš ï¸ æ¹¿åº¦ã®é…ã‚Œç›¸é–¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: %v", err)
	} else {
		// Factoråã« "humidity_" ã‚’è¿½åŠ 
		for i := range humLaggedCorrs {
			humLaggedCorrs[i].Factor = fmt.Sprintf("humidity_%s", humLaggedCorrs[i].Factor)
		}
		// çµ±è¨ˆçš„ã«æœ‰æ„ãªçµæœã®ã¿ã‚’è¿½åŠ 
		for _, corr := range humLaggedCorrs {
			if corr.PValue < 0.05 || math.Abs(corr.CorrelationCoef) >= 0.3 {
				allResults = append(allResults, corr)
			}
		}
		log.Printf("âœ… æ¹¿åº¦ã®é…ã‚Œç›¸é–¢åˆ†æå®Œäº†: %dä»¶ã®æœ‰æ„ãªç›¸é–¢ã‚’æ¤œå‡º", len(humLaggedCorrs))
	}

	// ç›¸é–¢ä¿‚æ•°ã®çµ¶å¯¾å€¤ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
	sort.Slice(allResults, func(i, j int) bool {
		return math.Abs(allResults[i].CorrelationCoef) > math.Abs(allResults[j].CorrelationCoef)
	})

	// ä¸Šä½3ä»¶ã®ã¿ã‚’è¿”ã™ï¼ˆæœ€ã‚‚æœ‰æ„ãªç›¸é–¢ã®ã¿ã‚’è¡¨ç¤ºï¼‰
	if len(allResults) > 3 {
		allResults = allResults[:3]
		log.Printf("ğŸ“Š æ°—è±¡ãƒ‡ãƒ¼ã‚¿ç›¸é–¢: ä¸Šä½3ä»¶ã«çµã‚Šè¾¼ã¿ã¾ã—ãŸ")
	}

	return allResults, nil
}

// AnalyzeSalesEconomicCorrelation è²©å£²ãƒ‡ãƒ¼ã‚¿ã¨çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’åˆ†æï¼ˆé…ã‚Œç›¸é–¢ã‚’å«ã‚€ï¼‰
func (s *StatisticsService) AnalyzeSalesEconomicCorrelation(
	salesData []models.WeatherSalesData,
	symbols []string,
	maxLagDays int,
) ([]models.CorrelationResult, error) {

	if len(salesData) == 0 {
		return nil, fmt.Errorf("è²©å£²ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
	}

	if s.economicService == nil {
		log.Printf("âš ï¸ EconomicService ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
		return []models.CorrelationResult{}, nil
	}

	// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ãƒ³ãƒœãƒ«ãƒªã‚¹ãƒˆ
	if len(symbols) == 0 {
		symbols = []string{"NIKKEI", "USDJPY", "WTI"}
	}

	// è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ç¯„å›²ã‚’ç‰¹å®š
	var startDate, endDate time.Time
	for i, data := range salesData {
		t, err := time.Parse("2006-01-02", data.Date)
		if err != nil {
			continue
		}
		if i == 0 || t.Before(startDate) {
			startDate = t
		}
		if i == 0 || t.After(endDate) {
			endDate = t
		}
	}

	// æ—¥ä»˜ç¯„å›²ãŒç‰¹å®šã§ããªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆéå»90æ—¥ï¼‰
	if startDate.IsZero() || endDate.IsZero() {
		endDate = time.Now()
		startDate = endDate.AddDate(0, 0, -90)
	}

	// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ©ã‚°ç¯„å›²
	if maxLagDays == 0 {
		maxLagDays = 30 // æœ€å¤§30æ—¥ã®é…ã‚Œç›¸é–¢ã‚’èª¿ã¹ã‚‹
	}

	var allResults []models.CorrelationResult

	// å„çµŒæ¸ˆæŒ‡æ¨™ã«ã¤ã„ã¦ç›¸é–¢ã‚’è¨ˆç®—
	for _, symbol := range symbols {
		// çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
		economicSeries, err := s.economicService.GetMarketSeries(symbol, startDate, endDate)
		if err != nil {
			log.Printf("âš ï¸ çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ (%s): %v", symbol, err)
			continue
		}

		if len(economicSeries) == 0 {
			log.Printf("âš ï¸ çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ (%s)", symbol)
			continue
		}

		// çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ—åŒ–
		econMap := make(map[string]float64)
		for _, point := range economicSeries {
			econMap[point.Date.Format("2006-01-02")] = point.Value
		}

		// è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã¨å€¤ã‚’æŠ½å‡º
		var salesDates []string
		var salesValues []float64
		for _, sale := range salesData {
			salesDates = append(salesDates, sale.Date)
			salesValues = append(salesValues, sale.Sales)
		}

		// çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã¨å€¤ã‚’æŠ½å‡º
		var econDates []string
		var econValues []float64
		for _, point := range economicSeries {
			econDates = append(econDates, point.Date.Format("2006-01-02"))
			econValues = append(econValues, point.Value)
		}

		// é…ã‚Œç›¸é–¢ã‚’è¨ˆç®—
		laggedCorrs, err := s.CalculateLaggedCorrelations(salesDates, salesValues, econDates, econValues, maxLagDays)
		if err != nil {
			log.Printf("âš ï¸ é…ã‚Œç›¸é–¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼ (%s): %v", symbol, err)
			continue
		}

		// ã‚·ãƒ³ãƒœãƒ«åã‚’å„ç›¸é–¢çµæœã«è¿½åŠ 
		for i := range laggedCorrs {
			// Factoråã‚’æ›´æ–°ï¼ˆã‚·ãƒ³ãƒœãƒ«åã‚’å«ã‚ã‚‹ï¼‰
			laggedCorrs[i].Factor = fmt.Sprintf("%s_%s", symbol, laggedCorrs[i].Factor)
		}

		// çµ±è¨ˆçš„ã«æœ‰æ„ãªçµæœï¼ˆp < 0.05ï¼‰ã®ã¿ã‚’è¿½åŠ 
		// ã¾ãŸã¯çµ¶å¯¾ç›¸é–¢ä¿‚æ•°ãŒ0.3ä»¥ä¸Šã®ã‚‚ã®ã‚’è¿½åŠ 
		for _, corr := range laggedCorrs {
			if corr.PValue < 0.05 || math.Abs(corr.CorrelationCoef) >= 0.3 {
				allResults = append(allResults, corr)
			}
		}

		log.Printf("âœ… çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ç›¸é–¢åˆ†æå®Œäº† (%s): %dä»¶ã®æœ‰æ„ãªç›¸é–¢ã‚’æ¤œå‡º", symbol, len(laggedCorrs))
	}

	// ç›¸é–¢ä¿‚æ•°ã®çµ¶å¯¾å€¤ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
	sort.Slice(allResults, func(i, j int) bool {
		return math.Abs(allResults[i].CorrelationCoef) > math.Abs(allResults[j].CorrelationCoef)
	})

	// ä¸Šä½3ä»¶ã®ã¿ã‚’è¿”ã™ï¼ˆæœ€ã‚‚æœ‰æ„ãªç›¸é–¢ã®ã¿ã‚’è¡¨ç¤ºï¼‰
	if len(allResults) > 3 {
		allResults = allResults[:3]
		log.Printf("ğŸ“Š çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ç›¸é–¢: ä¸Šä½3ä»¶ã«çµã‚Šè¾¼ã¿ã¾ã—ãŸ")
	}

	return allResults, nil
}

// PerformLinearRegression å˜å›å¸°åˆ†æã‚’å®Ÿè¡Œ
func (s *StatisticsService) PerformLinearRegression(x, y []float64) (*models.RegressionResult, error) {
	if len(x) != len(y) || len(x) < 2 {
		return nil, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
	}

	n := float64(len(x))
	var sumX, sumY, sumXY, sumX2 float64

	for i := 0; i < len(x); i++ {
		sumX += x[i]
		sumY += y[i]
		sumXY += x[i] * y[i]
		sumX2 += x[i] * x[i]
	}

	// å‚¾ãï¼ˆslopeï¼‰ã®è¨ˆç®—
	slope := (n*sumXY - sumX*sumY) / (n*sumX2 - sumX*sumX)

	// åˆ‡ç‰‡ï¼ˆinterceptï¼‰ã®è¨ˆç®—
	intercept := (sumY - slope*sumX) / n

	// RÂ²ï¼ˆæ±ºå®šä¿‚æ•°ï¼‰ã®è¨ˆç®—
	meanY := sumY / n
	var ssTotal, ssResidual float64
	for i := 0; i < len(x); i++ {
		predicted := slope*x[i] + intercept
		ssTotal += (y[i] - meanY) * (y[i] - meanY)
		ssResidual += (y[i] - predicted) * (y[i] - predicted)
	}
	rSquared := 1 - (ssResidual / ssTotal)

	// äºˆæ¸¬å€¤ã®è¨ˆç®—ï¼ˆæœ€å¾Œã®xå€¤ã‚’ä½¿ç”¨ï¼‰
	lastX := x[len(x)-1]
	prediction := slope*lastX + intercept

	// ä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆRÂ²ã‚’ãƒ™ãƒ¼ã‚¹ã«ï¼‰
	confidence := rSquared

	description := fmt.Sprintf("å›å¸°å¼: y = %.2fx + %.2f (RÂ² = %.3f)", slope, intercept, rSquared)

	return &models.RegressionResult{
		Slope:       slope,
		Intercept:   intercept,
		RSquared:    rSquared,
		Prediction:  prediction,
		Confidence:  confidence,
		Description: description,
	}, nil
}

// GenerateStatisticalSummary çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
func (s *StatisticsService) GenerateStatisticalSummary(
	salesData []models.WeatherSalesData,
	regionCode string,
) (string, error) {

	if len(salesData) == 0 {
		return "", fmt.Errorf("è²©å£²ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
	}

	// åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—
	var totalSales, minSales, maxSales float64
	minSales = math.MaxFloat64
	maxSales = -math.MaxFloat64

	salesByDate := make(map[string]float64)
	for _, data := range salesData {
		totalSales += data.Sales
		if data.Sales < minSales {
			minSales = data.Sales
		}
		if data.Sales > maxSales {
			maxSales = data.Sales
		}
		salesByDate[data.Date] = data.Sales
	}

	avgSales := totalSales / float64(len(salesData))

	// æ¨™æº–åå·®ã®è¨ˆç®—
	var variance float64
	for _, data := range salesData {
		diff := data.Sales - avgSales
		variance += diff * diff
	}
	stdDev := math.Sqrt(variance / float64(len(salesData)))

	// ä¸­å¤®å€¤ã®è¨ˆç®—
	sortedSales := make([]float64, len(salesData))
	for i, data := range salesData {
		sortedSales[i] = data.Sales
	}
	sort.Float64s(sortedSales)
	median := sortedSales[len(sortedSales)/2]

	summary := fmt.Sprintf(`çµ±è¨ˆã‚µãƒãƒªãƒ¼:
- ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: %d
- å¹³å‡å£²ä¸Š: %.2f
- ä¸­å¤®å€¤: %.2f
- æ¨™æº–åå·®: %.2f
- æœ€å°å€¤: %.2f
- æœ€å¤§å€¤: %.2f
- ç·å£²ä¸Š: %.2f
`,
		len(salesData),
		avgSales,
		median,
		stdDev,
		minSales,
		maxSales,
		totalSales,
	)

	return summary, nil
}

// CreateAnalysisReport ç·åˆçš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
func (s *StatisticsService) CreateAnalysisReport(
	fileName string,
	salesData []models.WeatherSalesData,
	regionCode string,
	aiInsights string,
) (*models.AnalysisReport, error) {

	// ç›¸é–¢åˆ†æï¼ˆå¤©æ°—ãƒ‡ãƒ¼ã‚¿ï¼‰
	weatherCorrelations, err := s.AnalyzeSalesWeatherCorrelation(salesData, regionCode)
	if err != nil {
		weatherCorrelations = []models.CorrelationResult{} // ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç©ºé…åˆ—ã§ç¶™ç¶š
	}

	// ç›¸é–¢åˆ†æï¼ˆçµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ï¼‰- é…ã‚Œç›¸é–¢ã‚‚å«ã‚€
	economicCorrelations, err := s.AnalyzeSalesEconomicCorrelation(salesData, []string{"NIKKEI", "USDJPY", "WTI"}, 30)
	if err != nil {
		log.Printf("âš ï¸ çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ç›¸é–¢åˆ†æã‚¨ãƒ©ãƒ¼: %v", err)
		economicCorrelations = []models.CorrelationResult{}
	}

	// å¤©æ°—ã¨çµŒæ¸ˆã®ç›¸é–¢çµæœã‚’çµåˆ
	correlations := append(weatherCorrelations, economicCorrelations...)

	// çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ
	summary, err := s.GenerateStatisticalSummary(salesData, regionCode)
	if err != nil {
		summary = "çµ±è¨ˆã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
	}

	// å›å¸°åˆ†æï¼ˆæ°—æ¸©ã¨å£²ä¸Šï¼‰
	var regression *models.RegressionResult
	var weatherMatches int
	var dateRange string

	if len(salesData) > 0 {
		// è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ç¯„å›²ã‚’ç‰¹å®š
		var startDate, endDate time.Time
		for i, data := range salesData {
			t, err := time.Parse("2006-01-02", data.Date)
			if err != nil {
				continue
			}
			if i == 0 || t.Before(startDate) {
				startDate = t
			}
			if i == 0 || t.After(endDate) {
				endDate = t
			}
		}

		// æ—¥ä»˜ç¯„å›²ãŒç‰¹å®šã§ããªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
		if startDate.IsZero() || endDate.IsZero() {
			endDate = time.Now()
			startDate = endDate.AddDate(0, 0, -90)
		}

		dateRange = fmt.Sprintf("%s ã€œ %s", startDate.Format("2006-01-02"), endDate.Format("2006-01-02"))

		// æ°—æ¸©ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
		var temps, sales []float64
		weatherData, err := s.weatherService.GetHistoricalWeatherData(regionCode, startDate, endDate)
		if err != nil {
			log.Printf("âš ï¸ æ°—è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: %v", err)
		} else {
			log.Printf("âœ… æ°—è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: %dä»¶ (æœŸé–“: %s ã€œ %s)", len(weatherData), startDate.Format("2006-01-02"), endDate.Format("2006-01-02"))
		}

		weatherMap := make(map[string]float64)
		for _, w := range weatherData {
			weatherMap[w.Date] = w.Temperature
		}

		log.Printf("ğŸ“Š è²©å£²ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: %d, æ°—è±¡ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: %d", len(salesData), len(weatherMap))

		// æ—¥ä»˜å½¢å¼ã®è¨ºæ–­ãƒ­ã‚°ã‚’è¿½åŠ 
		if len(salesData) > 0 {
			log.Printf("ğŸ” [è¨ºæ–­] è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ä¾‹: '%s'", salesData[0].Date)
		}
		if len(weatherData) > 0 {
			log.Printf("ğŸ” [è¨ºæ–­] æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ä¾‹: '%s'", weatherData[0].Date)
		}

		for _, sale := range salesData {
			if temp, ok := weatherMap[sale.Date]; ok {
				temps = append(temps, temp)
				sales = append(sales, sale.Sales)
				weatherMatches++
			}
		}

		log.Printf("ğŸ”— ãƒãƒƒãƒãƒ³ã‚°çµæœ: %dä»¶ / %dä»¶", weatherMatches, len(salesData))

		if len(temps) >= 2 {
			regression, _ = s.PerformLinearRegression(temps, sales)
		}
	}

	// ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
	recommendations := s.generateRecommendations(correlations, regression)

	report := &models.AnalysisReport{
		ReportID:        uuid.New().String(),
		FileName:        fileName,
		AnalysisDate:    time.Now().Format(time.RFC3339),
		DataPoints:      len(salesData),
		DateRange:       dateRange,
		WeatherMatches:  weatherMatches,
		Summary:         summary,
		Correlations:    correlations,
		Regression:      regression,
		AIInsights:      aiInsights,
		Recommendations: recommendations,
	}

	return report, nil
}

// generateRecommendations åˆ†æçµæœã«åŸºã¥ã„ã¦ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
func (s *StatisticsService) generateRecommendations(
	correlations []models.CorrelationResult,
	regression *models.RegressionResult,
) []string {
	var recommendations []string

	// ç›¸é–¢åˆ†æã‹ã‚‰ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¤©æ°—ãƒ‡ãƒ¼ã‚¿ï¼‰
	for _, corr := range correlations {
		if math.Abs(corr.CorrelationCoef) > 0.5 && corr.PValue < 0.05 {
			if corr.Factor == "temperature" {
				if corr.CorrelationCoef > 0 {
					recommendations = append(recommendations, "æ°—æ¸©ãŒé«˜ã„ã»ã©å£²ä¸ŠãŒå¢—åŠ ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚å¤å­£ã®åœ¨åº«ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
				} else {
					recommendations = append(recommendations, "æ°—æ¸©ãŒä½ã„ã»ã©å£²ä¸ŠãŒå¢—åŠ ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚å†¬å­£ã®åœ¨åº«ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
				}
			}
			if corr.Factor == "humidity" {
				recommendations = append(recommendations, "æ¹¿åº¦ã¨å£²ä¸Šã«æœ‰æ„ãªç›¸é–¢ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚å¤©æ°—äºˆå ±ã¨é€£å‹•ã—ãŸåœ¨åº«ç®¡ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
			}
			// çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢
			if strings.Contains(corr.Factor, "NIKKEI") {
				if corr.CorrelationCoef > 0 {
					recommendations = append(recommendations, fmt.Sprintf("æ—¥çµŒå¹³å‡ã¨ã®æ­£ã®ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚æ ªä¾¡å‹•å‘ã‚’éœ€è¦äºˆæ¸¬ã«æ´»ç”¨ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", corr.CorrelationCoef))
				} else {
					recommendations = append(recommendations, fmt.Sprintf("æ—¥çµŒå¹³å‡ã¨ã®è² ã®ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚æ™¯æ°—å¾Œé€€æœŸã«éœ€è¦ãŒå¢—åŠ ã™ã‚‹è£½å“ç‰¹æ€§ãŒç¤ºå”†ã•ã‚Œã¾ã™ã€‚", corr.CorrelationCoef))
				}
			}
			if strings.Contains(corr.Factor, "USDJPY") {
				recommendations = append(recommendations, fmt.Sprintf("ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆï¼ˆUSD/JPYï¼‰ã¨ã®ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚è¼¸å…¥åŸææ–™ã‚„å¤–å›½äººè¦³å…‰å®¢éœ€è¦ã®å½±éŸ¿ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚", corr.CorrelationCoef))
			}
			if strings.Contains(corr.Factor, "WTI") {
				recommendations = append(recommendations, fmt.Sprintf("åŸæ²¹ä¾¡æ ¼ã¨ã®ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚è¼¸é€ã‚³ã‚¹ãƒˆã‚„æ¶ˆè²»è€…å¿ƒç†ã¸ã®å½±éŸ¿ã‚’ç›£è¦–ã—ã¦ãã ã•ã„ã€‚", corr.CorrelationCoef))
			}
		}
	}

	// é…ã‚Œç›¸é–¢ã®æ¤œå‡º
	for _, corr := range correlations {
		if strings.Contains(corr.Factor, "é…ã‚Œ") || strings.Contains(corr.Factor, "å…ˆè¡Œ") {
			if math.Abs(corr.CorrelationCoef) > 0.4 && corr.PValue < 0.05 {
				recommendations = append(recommendations, fmt.Sprintf("â±ï¸ ã‚¿ã‚¤ãƒ ãƒ©ã‚°ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: %sï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚å…ˆè¡ŒæŒ‡æ¨™ã¨ã—ã¦æ´»ç”¨ã§ãã¾ã™ã€‚", corr.Factor, corr.CorrelationCoef))
			}
		}
	}

	// å›å¸°åˆ†æã‹ã‚‰ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
	if regression != nil && regression.RSquared > 0.3 {
		recommendations = append(recommendations, fmt.Sprintf("å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¯%.1f%%ã§ã™ã€‚æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸéœ€è¦äºˆæ¸¬ãŒæœ‰åŠ¹ã§ã™ã€‚", regression.RSquared*100))
	}

	// ç›¸é–¢ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
	if len(correlations) == 0 {
		recommendations = append(recommendations, "âš ï¸ è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã¨å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãŒãƒãƒƒãƒã—ã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆYYYY-MM-DDå½¢å¼ã‚’æ¨å¥¨ï¼‰ã€‚")
		recommendations = append(recommendations, "ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æœŸé–“ã¨ã®æ•´åˆæ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
	}

	// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
	if len(recommendations) == 0 {
		recommendations = append(recommendations, "ã•ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿è“„ç©ã«ã‚ˆã‚Šã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚")
		recommendations = append(recommendations, "å­£ç¯€æ€§ã‚„æ›œæ—¥åŠ¹æœã‚‚è€ƒæ…®ã—ãŸå¤šå¤‰é‡è§£æã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
	}

	return recommendations
}

// PredictFutureSales å°†æ¥ã®å£²ä¸Šã‚’äºˆæ¸¬ã™ã‚‹
func (s *StatisticsService) PredictFutureSales(
	historicalSales []float64,
	historicalTemperatures []float64,
	futureTemperature float64,
	confidenceLevel float64,
) (models.SalesPrediction, error) {
	if len(historicalSales) != len(historicalTemperatures) {
		return models.SalesPrediction{}, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“")
	}

	if len(historicalSales) < 10 {
		return models.SalesPrediction{}, fmt.Errorf("äºˆæ¸¬ã«ã¯æœ€ä½10ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
	}

	// 1. å›å¸°åˆ†æã§äºˆæ¸¬å€¤ã‚’è¨ˆç®—
	regression, err := s.PerformLinearRegression(historicalTemperatures, historicalSales)
	if err != nil {
		return models.SalesPrediction{}, err
	}

	predictedValue := regression.Slope*futureTemperature + regression.Intercept

	// 2. æ®‹å·®ã®æ¨™æº–åå·®ã‚’è¨ˆç®—ï¼ˆäºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ï¼‰
	var residuals []float64
	for i := 0; i < len(historicalSales); i++ {
		predicted := regression.Slope*historicalTemperatures[i] + regression.Intercept
		residual := historicalSales[i] - predicted
		residuals = append(residuals, residual)
	}

	residualStdDev := s.calculateStandardDeviation(residuals)

	// 3. ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ95%ï¼‰
	if confidenceLevel == 0 {
		confidenceLevel = 0.95
	}

	// zå€¤ï¼ˆæ­£è¦åˆ†å¸ƒï¼‰: 90%=1.645, 95%=1.96, 99%=2.576
	var zScore float64
	switch confidenceLevel {
	case 0.90:
		zScore = 1.645
	case 0.95:
		zScore = 1.96
	case 0.99:
		zScore = 2.576
	default:
		zScore = 1.96 // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ95%
	}

	margin := zScore * residualStdDev
	lowerBound := predictedValue - margin
	upperBound := predictedValue + margin

	// 4. äºˆæ¸¬ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—ï¼ˆRÂ²å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
	confidence := regression.RSquared

	// 5. äºˆæ¸¬æ ¹æ‹ ã‚’ç”Ÿæˆ
	factors := []string{
		fmt.Sprintf("æ°—æ¸© %.1fÂ°C ã«åŸºã¥ãå›å¸°äºˆæ¸¬", futureTemperature),
		fmt.Sprintf("éå» %d ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’", len(historicalSales)),
		fmt.Sprintf("æ±ºå®šä¿‚æ•° RÂ² = %.3f", regression.RSquared),
	}

	if regression.RSquared > 0.5 {
		factors = append(factors, "æ°—æ¸©ã¨å£²ä¸Šã®ç›¸é–¢ãŒå¼·ã„ãŸã‚ã€äºˆæ¸¬ç²¾åº¦ã¯é«˜ã„ã§ã™")
	} else if regression.RSquared > 0.3 {
		factors = append(factors, "æ°—æ¸©ã¨å£²ä¸Šã«ç›¸é–¢ãŒã‚ã‚Šã¾ã™ãŒã€ä»–ã®è¦å› ã‚‚è€ƒæ…®ãŒå¿…è¦ã§ã™")
	} else {
		factors = append(factors, "æ°—æ¸©ä»¥å¤–ã®è¦å› ãŒå£²ä¸Šã«å¤§ããå½±éŸ¿ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
	}

	return models.SalesPrediction{
		PredictedValue: predictedValue,
		ConfidenceInterval: models.ConfidenceInterval{
			Lower:      lowerBound,
			Upper:      upperBound,
			Confidence: confidenceLevel,
		},
		Confidence:         confidence,
		PredictionFactors:  factors,
		RegressionEquation: fmt.Sprintf("y = %.2fx + %.2f", regression.Slope, regression.Intercept),
	}, nil
}

// DetectAnomalies å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç•°å¸¸å€¤ã‚’æ¤œå‡ºã™ã‚‹ï¼ˆç§»å‹•å¹³å‡ä¹–é›¢ç‡æ³•ï¼‰
// granularity: "daily", "weekly", "monthly" - ãƒ‡ãƒ¼ã‚¿é›†ç´„ç²’åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "weekly"ï¼‰
func (s *StatisticsService) DetectAnomalies(sales []float64, dates []string, productID string, productName string) []models.AnomalyDetection {
	return s.DetectAnomaliesWithGranularity(sales, dates, productID, productName, "weekly")
}

// DetectAnomaliesWithGranularity ç²’åº¦ã‚’æŒ‡å®šã—ã¦ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè¡Œ
func (s *StatisticsService) DetectAnomaliesWithGranularity(sales []float64, dates []string, productID string, productName string, granularity string) []models.AnomalyDetection {
	displayName := productName
	if displayName == "" {
		displayName = productID
	}

	// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é€±æ¬¡
	if granularity == "" {
		granularity = "weekly"
	}

	log.Printf("[ç•°å¸¸æ¤œçŸ¥@%s] ç²’åº¦: %s ã§ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã—ã¦ã‹ã‚‰ç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè¡Œã—ã¾ã™", displayName, granularity)

	// æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®ã¿é›†ç´„ãŒå¿…è¦ï¼ˆé€±æ¬¡ãƒ»æœˆæ¬¡ã®å ´åˆã¯æ—¢ã«é›†ç´„æ¸ˆã¿ã¨ä»®å®šï¼‰
	aggregatedSales := sales
	aggregatedDates := dates

	if granularity != "daily" && len(sales) > 0 {
		// ãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡ã¾ãŸã¯æœˆæ¬¡ã«é›†ç´„
		aggregatedSales, aggregatedDates = s.aggregateDataForAnomalyDetection(sales, dates, granularity)
		log.Printf("[ç•°å¸¸æ¤œçŸ¥@%s] ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„: %dä»¶ â†’ %dä»¶", displayName, len(sales), len(aggregatedSales))
	}

	// ç§»å‹•å¹³å‡ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’ç²’åº¦ã«å¿œã˜ã¦èª¿æ•´
	var windowSize int
	var percentageThreshold float64

	switch granularity {
	case "daily":
		windowSize = 30           // 30æ—¥é–“ã®ç§»å‹•å¹³å‡
		percentageThreshold = 0.5 // 50%ã®ä¹–é›¢
	case "weekly":
		windowSize = 4            // 4é€±é–“ã®ç§»å‹•å¹³å‡
		percentageThreshold = 0.4 // 40%ã®ä¹–é›¢ï¼ˆé€±æ¬¡ã¯å¤‰å‹•ãŒå¤§ãã„ãŸã‚ç·©å’Œï¼‰
	case "monthly":
		windowSize = 3            // 3ãƒ¶æœˆã®ç§»å‹•å¹³å‡
		percentageThreshold = 0.3 // 30%ã®ä¹–é›¢ï¼ˆæœˆæ¬¡ã¯ã•ã‚‰ã«ç·©å’Œï¼‰
	default:
		windowSize = 4
		percentageThreshold = 0.4
	}

	if len(aggregatedSales) < windowSize {
		log.Printf("[ç•°å¸¸æ¤œçŸ¥@%s] ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªãã€ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ï¼ˆ%dä»¶ < %dä»¶ï¼‰", displayName, len(aggregatedSales), windowSize)
		return []models.AnomalyDetection{}
	}

	var anomalies []models.AnomalyDetection

	for i := windowSize; i < len(aggregatedSales); i++ {
		// ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
		window := aggregatedSales[i-windowSize : i]

		// ç§»å‹•å¹³å‡ã‚’è¨ˆç®—
		mean := s.calculateMean(window)

		// ç¾åœ¨ã®å€¤
		currentValue := aggregatedSales[i]

		// ç§»å‹•å¹³å‡ã‹ã‚‰ã®ä¹–é›¢ã‚’è¨ˆç®—
		deviation := currentValue - mean

		// é–¾å€¤ã‚’è¨ˆç®—
		threshold := mean * percentageThreshold

		if mean > 0 && math.Abs(deviation) > threshold {
			anomalyType := "æ€¥å¢—"
			if deviation < 0 {
				anomalyType = "æ€¥æ¸›"
			}

			// Zã‚¹ã‚³ã‚¢ã¯å‚è€ƒå€¤ã¨ã—ã¦ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®çµ±è¨ˆã§è¨ˆç®—ï¼‰
			stdDev := s.calculateStandardDeviation(window)
			var zScore float64
			if stdDev > 0 {
				zScore = deviation / stdDev
			}

			anomalies = append(anomalies, models.AnomalyDetection{
				Date:          aggregatedDates[i],
				ProductID:     productID,
				ProductName:   productName,
				ActualValue:   currentValue,
				ExpectedValue: mean, // æœŸå¾…å€¤ã¨ã—ã¦ç§»å‹•å¹³å‡ã‚’ä½¿ç”¨
				Deviation:     math.Abs(deviation),
				ZScore:        zScore,
				AnomalyType:   anomalyType,
				Severity:      s.calculateSeverity(math.Abs(zScore)),
			})
		}
	}

	log.Printf("[ç•°å¸¸æ¤œçŸ¥@%s] ç§»å‹•å¹³å‡æ³•ã«ã‚ˆã‚Š %d ä»¶ã®ç•°å¸¸ã‚’æ¤œå‡ºã—ã¾ã—ãŸ", displayName, len(anomalies))

	return anomalies
}

// aggregateDataForAnomalyDetection ç•°å¸¸æ¤œçŸ¥ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
func (s *StatisticsService) aggregateDataForAnomalyDetection(sales []float64, dates []string, granularity string) ([]float64, []string) {
	if len(sales) != len(dates) {
		log.Printf("[è­¦å‘Š] sales ã¨ dates ã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“: %d != %d", len(sales), len(dates))
		return sales, dates
	}

	// æœŸé–“ã‚­ãƒ¼ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
	periodMap := make(map[string][]float64)
	periodOrder := []string{} // é †åºã‚’ä¿æŒ

	for i, dateStr := range dates {
		t, err := time.Parse("2006-01-02", dateStr)
		if err != nil {
			log.Printf("[è­¦å‘Š] æ—¥ä»˜ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: %s", dateStr)
			continue
		}

		var periodKey string
		switch granularity {
		case "weekly":
			// æœˆæ›œå§‹ã¾ã‚Šã®é€±ç•ªå·
			year, week := t.ISOWeek()
			periodKey = fmt.Sprintf("%d-W%02d", year, week)
		case "monthly":
			periodKey = t.Format("2006-01")
		default:
			periodKey = dateStr // æ—¥æ¬¡ã®å ´åˆã¯ãã®ã¾ã¾
		}

		if _, exists := periodMap[periodKey]; !exists {
			periodOrder = append(periodOrder, periodKey)
		}
		periodMap[periodKey] = append(periodMap[periodKey], sales[i])
	}

	// é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
	aggregatedSales := make([]float64, 0, len(periodOrder))
	aggregatedDates := make([]string, 0, len(periodOrder))

	for _, periodKey := range periodOrder {
		values := periodMap[periodKey]

		// åˆè¨ˆã‚’è¨ˆç®—
		var total float64
		for _, v := range values {
			total += v
		}

		aggregatedSales = append(aggregatedSales, total)
		aggregatedDates = append(aggregatedDates, periodKey)
	}

	return aggregatedSales, aggregatedDates
}

// calculateSeverity ç•°å¸¸ã®æ·±åˆ»åº¦ã‚’è¨ˆç®—
func (s *StatisticsService) calculateSeverity(absZScore float64) string {
	if absZScore > 4.0 {
		return "critical" // æ¥µã‚ã¦ç•°å¸¸
	} else if absZScore > 3.5 {
		return "high" // é«˜åº¦ãªç•°å¸¸
	} else if absZScore > 3.0 {
		return "medium" // ä¸­ç¨‹åº¦ã®ç•°å¸¸
	}
	return "low"
}

// formatDateForDisplay æ—¥ä»˜ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
// ä¾‹: "2022-04" â†’ "2022å¹´4æœˆ"
//
//	"2022-W10" â†’ "2022å¹´ ç¬¬10é€±"
//	"2022-03-07" â†’ "2022å¹´3æœˆ7æ—¥"
func (s *StatisticsService) formatDateForDisplay(date string) string {
	// æœˆæ¬¡å½¢å¼: YYYY-MM
	if len(date) == 7 && date[4] == '-' {
		t, err := time.Parse("2006-01", date)
		if err == nil {
			return t.Format("2006å¹´1æœˆ")
		}
	}

	// é€±æ¬¡å½¢å¼: YYYY-WWW
	if len(date) >= 7 && strings.Contains(date, "-W") {
		parts := strings.Split(date, "-W")
		if len(parts) == 2 {
			return fmt.Sprintf("%så¹´ ç¬¬%sé€±", parts[0], parts[1])
		}
	}

	// æ—¥æ¬¡å½¢å¼: YYYY-MM-DD
	if len(date) == 10 {
		t, err := time.Parse("2006-01-02", date)
		if err == nil {
			return t.Format("2006å¹´1æœˆ2æ—¥")
		}
	}

	// ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
	return date
}

// GenerateAIQuestion ç•°å¸¸å€¤ã«åŸºã¥ã„ã¦AIãŒè³ªå•ã‚’ç”Ÿæˆ
func (s *StatisticsService) GenerateAIQuestion(anomaly models.AnomalyDetection) (string, []string) {
	// è£½å“ã®è¡¨ç¤ºåã‚’æ±ºå®šï¼ˆè£½å“åãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°IDï¼‰
	displayName := anomaly.ProductName
	if displayName == "" {
		displayName = anomaly.ProductID
	}

	// æ—¥ä»˜ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
	formattedDate := s.formatDateForDisplay(anomaly.Date)

	// AIã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ã€AIã«è³ªå•ã¨é¸æŠè‚¢ã‚’ç”Ÿæˆã•ã›ã‚‹
	if s.azureOpenAIService != nil {
		// AnomalyDetectionã‚’Anomalyã«å¤‰æ›ï¼ˆå¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ï¼‰
		anomalyForAI := models.Anomaly{
			Date:        formattedDate, // ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ã®æ—¥ä»˜ã‚’ä½¿ç”¨
			ProductID:   displayName,   // è¡¨ç¤ºåã‚’ä½¿ç”¨
			Description: fmt.Sprintf("å£²ä¸Š%s (å®Ÿç¸¾: %.0f, æœŸå¾…å€¤: %.0f)", anomaly.AnomalyType, anomaly.ActualValue, anomaly.ExpectedValue),
		}

		result, err := s.azureOpenAIService.GenerateQuestionAndChoicesFromAnomaly(anomalyForAI)
		if err == nil && result != nil && result.Question != "" {
			return result.Question, result.Choices
		}
		log.Printf("âš ï¸ AIã‹ã‚‰ã®è³ªå•ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: %v", err)
	}

	// ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®è³ªå•ã¨å›ºå®šã®é¸æŠè‚¢
	var question string
	if anomaly.AnomalyType == "æ€¥å¢—" {
		question = fmt.Sprintf(
			"ğŸ“ˆ %s ã«ã€Œ%sã€ã®å£²ä¸ŠãŒé€šå¸¸ã‚ˆã‚Š %.0f å¢—åŠ ã—ã¾ã—ãŸï¼ˆæœŸå¾…å€¤: %.0f â†’ å®Ÿç¸¾: %.0fï¼‰ã€‚ã“ã®æ™‚æœŸã«ç‰¹åˆ¥ãªã‚¤ãƒ™ãƒ³ãƒˆã€ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã€ã¾ãŸã¯å¤–çš„è¦å› ã¯ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ",
			formattedDate,
			displayName,
			anomaly.Deviation,
			anomaly.ExpectedValue,
			anomaly.ActualValue,
		)
	} else {
		question = fmt.Sprintf(
			"ğŸ“‰ %s ã«ã€Œ%sã€ã®å£²ä¸ŠãŒé€šå¸¸ã‚ˆã‚Š %.0f æ¸›å°‘ã—ã¾ã—ãŸï¼ˆæœŸå¾…å€¤: %.0f â†’ å®Ÿç¸¾: %.0fï¼‰ã€‚ã“ã®æ™‚æœŸã«å£²ä¸Šæ¸›å°‘ã®åŸå› ã¨ãªã£ãŸè¦å› ï¼ˆå¤©å€™ã€ç«¶åˆã€åœ¨åº«åˆ‡ã‚Œãªã©ï¼‰ã¯ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ",
			formattedDate,
			displayName,
			anomaly.Deviation,
			anomaly.ExpectedValue,
			anomaly.ActualValue,
		)
	}

	defaultChoices := []string{
		"ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ»è²©ä¿ƒæ´»å‹•",
		"å¤©å€™ã®å½±éŸ¿",
		"ç«¶åˆä»–ç¤¾ã®å‹•ã",
		"ç‰¹ã«æ€ã„å½“ãŸã‚‹ç¯€ã¯ãªã„",
		"ãã®ä»–ï¼ˆè‡ªç”±è¨˜è¿°ï¼‰",
	}

	return question, defaultChoices
}

// ForecastProductDemand è£½å“åˆ¥ã®éœ€è¦äºˆæ¸¬ã‚’å®Ÿè¡Œ
func (s *StatisticsService) ForecastProductDemand(
	productID string,
	productName string,
	historicalData []models.SalesDataPoint,
	period string,
	regionCode string,
) (models.ProductForecast, error) {
	if len(historicalData) < 14 {
		return models.ProductForecast{}, fmt.Errorf("äºˆæ¸¬ã«ã¯æœ€ä½14æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
	}

	// æœŸé–“ã®æ—¥æ•°ã‚’æ±ºå®š
	var forecastDays int
	switch period {
	case "week":
		forecastDays = 7
	case "2weeks":
		forecastDays = 14
	case "month":
		forecastDays = 30
	default:
		forecastDays = 7
	}

	// çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
	stats := s.calculateProductStatistics(historicalData)

	// æ›œæ—¥åŠ¹æœã‚’è¨ˆç®—
	weekdayEffect := s.calculateWeekdayEffect(historicalData)

	// æ°—æ¸©ã¨ã®ç›¸é–¢ã‚’è¨ˆç®—
	var temperatures, sales []float64
	for _, point := range historicalData {
		if point.Temperature > 0 {
			temperatures = append(temperatures, point.Temperature)
			sales = append(sales, point.Sales)
		}
	}

	var regression *models.RegressionResult
	var err error
	if len(temperatures) >= 10 {
		regression, err = s.PerformLinearRegression(temperatures, sales)
		if err != nil {
			log.Printf("å›å¸°åˆ†æã‚¨ãƒ©ãƒ¼: %v", err)
		}
	}

	// å°†æ¥ã®äºˆæ¸¬æ—¥ã‚’ç”Ÿæˆ
	lastDate, _ := time.Parse("2006-01-02", historicalData[len(historicalData)-1].Date)
	var dailyForecasts []models.DailyForecast
	var totalForecast float64

	for i := 1; i <= forecastDays; i++ {
		forecastDate := lastDate.AddDate(0, 0, i)
		dayOfWeek := s.getDayOfWeekJP(forecastDate.Weekday())

		// åŸºæº–å€¤: å…¨ä½“å¹³å‡
		baseValue := stats.Mean

		// æ›œæ—¥åŠ¹æœã‚’é©ç”¨
		if effect, ok := weekdayEffect[dayOfWeek]; ok {
			baseValue = baseValue * effect
		}

		// æ°—æ¸©åŠ¹æœã‚’é©ç”¨ï¼ˆå›å¸°ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰
		if regression != nil && regression.RSquared > 0.1 {
			// ç°¡æ˜“çš„ã«å­£ç¯€ã®å¹³å‡æ°—æ¸©ã‚’ä½¿ç”¨
			seasonalTemp := s.getSeasonalTemperature(forecastDate.Month())
			tempAdjustment := regression.Slope * (seasonalTemp - s.calculateMean(temperatures))
			baseValue += tempAdjustment
		}

		// ãƒˆãƒ¬ãƒ³ãƒ‰åŠ¹æœï¼ˆå˜ç´”ç§»å‹•å¹³å‡ã®å‚¾ãï¼‰
		trendAdjustment := s.calculateTrend(historicalData) * float64(i)
		baseValue += trendAdjustment

		dailyForecasts = append(dailyForecasts, models.DailyForecast{
			Date:           forecastDate.Format("2006-01-02"),
			DayOfWeek:      dayOfWeek,
			PredictedValue: math.Max(0, baseValue), // è² ã®å€¤ã‚’é¿ã‘ã‚‹
			Temperature:    s.getSeasonalTemperature(forecastDate.Month()),
		})

		totalForecast += baseValue
	}

	// ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—
	stdDev := stats.StdDev
	zScore := 1.96 // 95% confidence
	marginTotal := zScore * stdDev * math.Sqrt(float64(forecastDays))

	confidence := 0.5 // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
	if regression != nil {
		confidence = regression.RSquared
	}

	// æœŸé–“ã®ç¯„å›²ã‚’æ–‡å­—åˆ—åŒ–
	startForecast := dailyForecasts[0].Date
	endForecast := dailyForecasts[len(dailyForecasts)-1].Date
	forecastPeriod := fmt.Sprintf("%s ã€œ %s", startForecast, endForecast)

	// æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
	recommendations := s.generateForecastRecommendations(totalForecast, stats, period)

	// å­£ç¯€æ€§ã®åˆ¤å®š
	seasonality := s.detectSeasonality(historicalData)

	return models.ProductForecast{
		ProductID:      productID,
		ProductName:    productName,
		ForecastPeriod: forecastPeriod,
		PredictedTotal: math.Max(0, totalForecast),
		DailyAverage:   math.Max(0, totalForecast/float64(forecastDays)),
		ConfidenceInterval: models.ConfidenceInterval{
			Lower:      math.Max(0, totalForecast-marginTotal),
			Upper:      totalForecast + marginTotal,
			Confidence: 0.95,
		},
		Confidence:      confidence,
		DailyBreakdown:  dailyForecasts,
		Factors:         s.buildFactorsList(regression, weekdayEffect, stats),
		Seasonality:     seasonality,
		Recommendations: recommendations,
	}, nil
}

// calculateProductStatistics è£½å“ã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
func (s *StatisticsService) calculateProductStatistics(data []models.SalesDataPoint) models.ProductStatistics {
	var sales []float64
	weekdaySales := make(map[string][]float64)
	monthlySales := make(map[string][]float64)

	for _, point := range data {
		sales = append(sales, point.Sales)
		if point.DayOfWeek != "" {
			weekdaySales[point.DayOfWeek] = append(weekdaySales[point.DayOfWeek], point.Sales)
		}
		if t, err := time.Parse("2006-01-02", point.Date); err == nil {
			month := fmt.Sprintf("%dæœˆ", int(t.Month()))
			monthlySales[month] = append(monthlySales[month], point.Sales)
		}
	}

	mean := s.calculateMean(sales)
	stdDev := s.calculateStandardDeviation(sales)

	// æ›œæ—¥åˆ¥å¹³å‡
	weekdayAvg := make(map[string]float64)
	for day, values := range weekdaySales {
		weekdayAvg[day] = s.calculateMean(values)
	}

	// æœˆåˆ¥å¹³å‡
	monthlyAvg := make(map[string]float64)
	for month, values := range monthlySales {
		monthlyAvg[month] = s.calculateMean(values)
	}

	// ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ã‚’åˆ¤å®š
	trend := s.calculateTrend(data)
	var trendDirection string
	if trend > 0.5 {
		trendDirection = "å¢—åŠ "
	} else if trend < -0.5 {
		trendDirection = "æ¸›å°‘"
	} else {
		trendDirection = "å®‰å®š"
	}

	sortedSales := make([]float64, len(sales))
	copy(sortedSales, sales)
	sort.Float64s(sortedSales)

	median := sortedSales[len(sortedSales)/2]
	min := sortedSales[0]
	max := sortedSales[len(sortedSales)-1]

	return models.ProductStatistics{
		Mean:           mean,
		Median:         median,
		StdDev:         stdDev,
		Min:            min,
		Max:            max,
		WeekdayAverage: weekdayAvg,
		MonthlyAverage: monthlyAvg,
		TrendDirection: trendDirection,
	}
}

// calculateWeekdayEffect æ›œæ—¥åŠ¹æœã‚’è¨ˆç®—ï¼ˆå…¨ä½“å¹³å‡ã«å¯¾ã™ã‚‹æ¯”ç‡ï¼‰
func (s *StatisticsService) calculateWeekdayEffect(data []models.SalesDataPoint) map[string]float64 {
	weekdaySales := make(map[string][]float64)
	var allSales []float64

	for _, point := range data {
		allSales = append(allSales, point.Sales)
		if point.DayOfWeek != "" {
			weekdaySales[point.DayOfWeek] = append(weekdaySales[point.DayOfWeek], point.Sales)
		}
	}

	overallMean := s.calculateMean(allSales)
	effect := make(map[string]float64)

	for day, sales := range weekdaySales {
		dayMean := s.calculateMean(sales)
		effect[day] = dayMean / overallMean // 1.0ãŒå¹³å‡ã€>1.0ãŒå¹³å‡ä»¥ä¸Š
	}

	return effect
}

// calculateTrend å˜ç´”ãªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—ï¼ˆ1æ—¥ã‚ãŸã‚Šã®å¤‰åŒ–é‡ï¼‰
func (s *StatisticsService) calculateTrend(data []models.SalesDataPoint) float64 {
	if len(data) < 2 {
		return 0
	}

	// æœ€åˆã®1/3ã¨æœ€å¾Œã®1/3ã®å¹³å‡ã‚’æ¯”è¼ƒ
	n := len(data)
	firstThird := n / 3
	var earlySum, lateSum float64

	for i := 0; i < firstThird; i++ {
		earlySum += data[i].Sales
	}
	for i := n - firstThird; i < n; i++ {
		lateSum += data[i].Sales
	}

	earlyAvg := earlySum / float64(firstThird)
	lateAvg := lateSum / float64(firstThird)

	// 1æ—¥ã‚ãŸã‚Šã®å¤‰åŒ–é‡
	return (lateAvg - earlyAvg) / float64(n-firstThird)
}

// getSeasonalTemperature æœˆã”ã¨ã®å¹³å‡æ°—æ¸©ã‚’è¿”ã™ï¼ˆç°¡æ˜“ç‰ˆï¼‰
func (s *StatisticsService) getSeasonalTemperature(month time.Month) float64 {
	temps := map[time.Month]float64{
		time.January:   5.0,
		time.February:  6.0,
		time.March:     10.0,
		time.April:     15.0,
		time.May:       20.0,
		time.June:      24.0,
		time.July:      28.0,
		time.August:    29.0,
		time.September: 25.0,
		time.October:   19.0,
		time.November:  13.0,
		time.December:  7.0,
	}
	return temps[month]
}

// getDayOfWeekJP æ›œæ—¥ã‚’æ—¥æœ¬èªã§è¿”ã™
func (s *StatisticsService) getDayOfWeekJP(weekday time.Weekday) string {
	days := []string{"æ—¥", "æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ"}
	return days[int(weekday)]
}

// detectSeasonality å­£ç¯€æ€§ã‚’æ¤œå‡º
func (s *StatisticsService) detectSeasonality(data []models.SalesDataPoint) string {
	if len(data) < 30 {
		return ""
	}

	monthlySales := make(map[int][]float64)
	for _, point := range data {
		if t, err := time.Parse("2006-01-02", point.Date); err == nil {
			month := int(t.Month())
			monthlySales[month] = append(monthlySales[month], point.Sales)
		}
	}

	// å¤å­£(6-8æœˆ)ã¨å†¬å­£(12-2æœˆ)ã®å¹³å‡ã‚’æ¯”è¼ƒ
	var summerSum, winterSum float64
	var summerCount, winterCount int

	for month, sales := range monthlySales {
		avg := s.calculateMean(sales)
		if month >= 6 && month <= 8 {
			summerSum += avg
			summerCount++
		} else if month == 12 || month <= 2 {
			winterSum += avg
			winterCount++
		}
	}

	if summerCount > 0 && winterCount > 0 {
		summerAvg := summerSum / float64(summerCount)
		winterAvg := winterSum / float64(winterCount)

		diff := (summerAvg - winterAvg) / winterAvg * 100
		if diff > 20 {
			return fmt.Sprintf("å¤å­£éœ€è¦ãŒé«˜ã„å‚¾å‘ï¼ˆå†¬å­£æ¯” +%.0f%%ï¼‰", diff)
		} else if diff < -20 {
			return fmt.Sprintf("å†¬å­£éœ€è¦ãŒé«˜ã„å‚¾å‘ï¼ˆå¤å­£æ¯” +%.0f%%ï¼‰", -diff)
		}
	}

	return "æ˜ç¢ºãªå­£ç¯€æ€§ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
}

// generateForecastRecommendations äºˆæ¸¬ã«åŸºã¥ãæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
func (s *StatisticsService) generateForecastRecommendations(forecast float64, stats models.ProductStatistics, period string) []string {
	var recommendations []string

	// éœ€è¦ãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãæ¨å¥¨
	if forecast > stats.Mean*1.2 {
		recommendations = append(recommendations, fmt.Sprintf("äºˆæ¸¬éœ€è¦ãŒå¹³å‡ã‚ˆã‚Šé«˜ã„ã§ã™ã€‚ååˆ†ãªåœ¨åº«ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ï¼ˆäºˆæ¸¬: %.0f, å¹³å‡: %.0fï¼‰", forecast, stats.Mean))
	} else if forecast < stats.Mean*0.8 {
		recommendations = append(recommendations, "äºˆæ¸¬éœ€è¦ãŒå¹³å‡ã‚ˆã‚Šä½ã„ã§ã™ã€‚éå‰°åœ¨åº«ã«æ³¨æ„ã—ã¦ãã ã•ã„")
	}

	// æ›œæ—¥åŠ¹æœã«åŸºã¥ãæ¨å¥¨
	if len(stats.WeekdayAverage) > 0 {
		var maxDay string
		var maxValue float64
		for day, avg := range stats.WeekdayAverage {
			if avg > maxValue {
				maxValue = avg
				maxDay = day
			}
		}
		if maxDay != "" {
			recommendations = append(recommendations, fmt.Sprintf("%sæ›œæ—¥ã®éœ€è¦ãŒæœ€ã‚‚é«˜ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™", maxDay))
		}
	}

	// ãƒˆãƒ¬ãƒ³ãƒ‰ã«åŸºã¥ãæ¨å¥¨
	switch stats.TrendDirection {
	case "å¢—åŠ ":
		recommendations = append(recommendations, "éœ€è¦å¢—åŠ ãƒˆãƒ¬ãƒ³ãƒ‰ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ä¾›çµ¦ä½“åˆ¶ã®å¼·åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
	case "æ¸›å°‘":
		recommendations = append(recommendations, "éœ€è¦æ¸›å°‘ãƒˆãƒ¬ãƒ³ãƒ‰ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ–½ç­–ã®è¦‹ç›´ã—ã‚’æ¨å¥¨ã—ã¾ã™")
	}

	return recommendations
}

// buildFactorsList äºˆæ¸¬ã«ä½¿ç”¨ã—ãŸè¦å› ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
func (s *StatisticsService) buildFactorsList(regression *models.RegressionResult, weekdayEffect map[string]float64, stats models.ProductStatistics) []string {
	factors := []string{
		fmt.Sprintf("éå»ã®è²©å£²å®Ÿç¸¾ï¼ˆå¹³å‡: %.0få€‹/æ—¥ï¼‰", stats.Mean),
		fmt.Sprintf("ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘: %s", stats.TrendDirection),
	}

	if len(weekdayEffect) > 0 {
		factors = append(factors, "æ›œæ—¥ã«ã‚ˆã‚‹éœ€è¦å¤‰å‹•ã‚’è€ƒæ…®")
	}

	if regression != nil && regression.RSquared > 0.1 {
		factors = append(factors, fmt.Sprintf("æ°—æ¸©ã¨ã®ç›¸é–¢ï¼ˆRÂ² = %.2fï¼‰", regression.RSquared))
	}

	factors = append(factors, "å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ")

	return factors
}

// SimpleGrangerCausality performs a basic Granger causality test using OLS.
// direction: "x->y" tests whether past x helps predict y; order=lag order.
// Returns F-statistic and p-value (approximate).
func (s *StatisticsService) SimpleGrangerCausality(y []float64, x []float64, order int) (float64, float64, error) {
	n := len(y)
	if n != len(x) || n < 5 || order < 1 {
		return 0, 1, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¾ãŸã¯ä¸æ­£ãªorder")
	}
	// Build design matrices
	// Restricted model: y_t ~ y_{t-1..t-p}
	// Full model: y_t ~ y_{t-1..t-p} + x_{t-1..t-p}
	T := n - order
	if T <= order {
		return 0, 1, fmt.Errorf("ã‚µãƒ³ãƒ—ãƒ«ä¸è¶³")
	}
	// Construct matrices
	Y := make([]float64, T)
	Xr := make([][]float64, T)
	Xf := make([][]float64, T)
	for t := order; t < n; t++ {
		rowR := make([]float64, order)
		rowF := make([]float64, 2*order)
		for p := 1; p <= order; p++ {
			rowR[p-1] = y[t-p]
			rowF[p-1] = y[t-p]
			rowF[order+p-1] = x[t-p]
		}
		Xr[t-order] = rowR
		Xf[t-order] = rowF
		Y[t-order] = y[t]
	}
	rssR, err := olsRSS(Y, Xr)
	if err != nil {
		return 0, 1, err
	}
	rssF, err := olsRSS(Y, Xf)
	if err != nil {
		return 0, 1, err
	}
	// F-test
	df1 := float64(order)
	df2 := float64(T - 2*order)
	if df2 <= 0 {
		return 0, 1, fmt.Errorf("è‡ªç”±åº¦ä¸è¶³")
	}
	F := ((rssR - rssF) / df1) / (rssF / df2)
	// Approximate p-value using F distribution tail via incomplete beta (relation with Beta)
	p := fDistSurvival(F, df1, df2)
	return F, p, nil
}

// olsRSS fits linear regression without intercept using normal equations and returns residual sum of squares.
func olsRSS(y []float64, X [][]float64) (float64, error) {
	// Compute beta = (X'X)^{-1} X'y, then residuals y - X beta
	p := len(X[0])
	n := len(y)
	XtX := make([][]float64, p)
	for i := 0; i < p; i++ {
		XtX[i] = make([]float64, p)
	}
	Xty := make([]float64, p)
	for i := 0; i < n; i++ {
		xi := X[i]
		for a := 0; a < p; a++ {
			Xty[a] += xi[a] * y[i]
			for b := 0; b < p; b++ {
				XtX[a][b] += xi[a] * xi[b]
			}
		}
	}
	beta, err := solveSymmetric(XtX, Xty)
	if err != nil {
		return 0, err
	}
	// residuals
	var rss float64
	for i := 0; i < n; i++ {
		pred := 0.0
		for a := 0; a < p; a++ {
			pred += X[i][a] * beta[a]
		}
		e := y[i] - pred
		rss += e * e
	}
	return rss, nil
}

// solveSymmetric solves A x = b for symmetric positive definite matrix A using Cholesky; fall back to Gaussian elimination.
func solveSymmetric(A [][]float64, b []float64) ([]float64, error) {
	n := len(b)
	// Attempt Cholesky without goto
	L := make([][]float64, n)
	for i := 0; i < n; i++ {
		L[i] = make([]float64, n)
	}
	cholOK := true
	for i := 0; i < n && cholOK; i++ {
		for j := 0; j <= i; j++ {
			sum := 0.0
			for k := 0; k < j; k++ {
				sum += L[i][k] * L[j][k]
			}
			if i == j {
				v := A[i][i] - sum
				if v <= 0 {
					cholOK = false
					break
				}
				L[i][j] = math.Sqrt(v)
			} else {
				if L[j][j] == 0 {
					cholOK = false
					break
				}
				L[i][j] = (A[i][j] - sum) / L[j][j]
			}
		}
	}
	if cholOK {
		// Solve Ly = b
		y := make([]float64, n)
		for i := 0; i < n; i++ {
			sum := 0.0
			for k := 0; k < i; k++ {
				sum += L[i][k] * y[k]
			}
			y[i] = (b[i] - sum) / L[i][i]
		}
		// Solve L^T x = y
		x := make([]float64, n)
		for i := n - 1; i >= 0; i-- {
			sum := 0.0
			for k := i + 1; k < n; k++ {
				sum += L[k][i] * x[k]
			}
			x[i] = (y[i] - sum) / L[i][i]
		}
		return x, nil
	}
	// Gaussian elimination fallback
	Aug := make([][]float64, n)
	for i := 0; i < n; i++ {
		Aug[i] = make([]float64, n+1)
		copy(Aug[i], A[i])
		Aug[i][n] = b[i]
	}
	for i := 0; i < n; i++ {
		// pivot
		maxRow := i
		for r := i + 1; r < n; r++ {
			if math.Abs(Aug[r][i]) > math.Abs(Aug[maxRow][i]) {
				maxRow = r
			}
		}
		Aug[i], Aug[maxRow] = Aug[maxRow], Aug[i]
		if math.Abs(Aug[i][i]) < 1e-12 {
			return nil, fmt.Errorf("ç‰¹ç•°è¡Œåˆ—")
		}
		for r := i + 1; r < n; r++ {
			f := Aug[r][i] / Aug[i][i]
			for c := i; c <= n; c++ {
				Aug[r][c] -= f * Aug[i][c]
			}
		}
	}
	x2 := make([]float64, n)
	for i := n - 1; i >= 0; i-- {
		sum := 0.0
		for c := i + 1; c < n; c++ {
			sum += Aug[i][c] * x2[c]
		}
		x2[i] = (Aug[i][n] - sum) / Aug[i][i]
	}
	return x2, nil
}

// fDistSurvival computes survival function P(F_{d1,d2} >= f) using relation with regularized incomplete beta.
func fDistSurvival(f, d1, d2 float64) float64 {
	if f <= 0 {
		return 1
	}
	x := (d2) / (d2 + d1*f)
	// p = I_x(d2/2, d1/2)
	return regularizedIncompleteBeta(d2/2, d1/2, x)
}

// AnalyzeWeeklySales é€±æ¬¡å˜ä½ã§ã®è²©å£²åˆ†æï¼ˆç²’åº¦æŒ‡å®šå¯èƒ½ï¼‰
func (s *StatisticsService) AnalyzeWeeklySales(productID, productName string, salesData []models.SalesDataPoint, startDate, endDate time.Time, granularity string) (*models.WeeklyAnalysisResponse, error) {
	if len(salesData) == 0 {
		return nil, fmt.Errorf("è²©å£²ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
	}

	// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é€±æ¬¡
	if granularity == "" {
		granularity = "weekly"
	}

	var weeklySummaries []models.WeeklySummary

	switch granularity {
	case "daily":
		// æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ï¼ˆé›†ç´„ãªã—ï¼‰
		weeklySummaries = s.groupByDay(salesData)
	case "monthly":
		// æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿
		weeklySummaries = s.groupByMonth(salesData, startDate)
	default: // "weekly"
		// ãƒ‡ãƒ¼ã‚¿ã‚’é€±å˜ä½ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
		weeklyGroups := s.groupByWeek(salesData, startDate)

		// é€±ã”ã¨ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
		weeklySummaries = make([]models.WeeklySummary, 0)
		var prevWeekSales float64 = 0

		for weekNum := 0; weekNum < len(weeklyGroups); weekNum++ {
			weekData, exists := weeklyGroups[weekNum]
			if !exists {
				continue
			}
			summary := s.calculateWeeklySummary(weekNum, weekData, prevWeekSales)
			weeklySummaries = append(weeklySummaries, summary)
			prevWeekSales = summary.TotalSales
		}
	}

	// å…¨ä½“çµ±è¨ˆã‚’è¨ˆç®—
	overallStats := s.calculateWeeklyOverallStats(weeklySummaries)

	// ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
	trends := s.analyzeWeeklyTrends(weeklySummaries)

	// æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
	recommendations := s.generateWeeklyRecommendations(weeklySummaries, overallStats, trends)

	return &models.WeeklyAnalysisResponse{
		ProductID:       productID,
		ProductName:     productName,
		AnalysisPeriod:  fmt.Sprintf("%s ~ %s", startDate.Format("2006-01-02"), endDate.Format("2006-01-02")),
		TotalWeeks:      len(weeklySummaries),
		WeeklySummary:   weeklySummaries,
		OverallStats:    overallStats,
		Trends:          trends,
		Recommendations: recommendations,
		Granularity:     granularity,
	}, nil
}

// groupByWeek ãƒ‡ãƒ¼ã‚¿ã‚’é€±å˜ä½ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆæœˆæ›œå§‹ã¾ã‚Šï¼‰
func (s *StatisticsService) groupByWeek(data []models.SalesDataPoint, startDate time.Time) map[int][]models.SalesDataPoint {
	weeklyGroups := make(map[int][]models.SalesDataPoint)

	for _, point := range data {
		date, err := time.Parse("2006-01-02", point.Date)
		if err != nil {
			continue
		}

		// é–‹å§‹æ—¥ã‹ã‚‰ã®é€±æ•°ã‚’è¨ˆç®—ï¼ˆæœˆæ›œå§‹ã¾ã‚Šï¼‰
		weekNum := s.getWeekNumber(date, startDate)
		weeklyGroups[weekNum] = append(weeklyGroups[weekNum], point)
	}

	return weeklyGroups
}

// groupByDay ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥æ¬¡ã§ã‚µãƒãƒªãƒ¼åŒ–ï¼ˆé›†ç´„ãªã—ï¼‰
func (s *StatisticsService) groupByDay(data []models.SalesDataPoint) []models.WeeklySummary {
	summaries := make([]models.WeeklySummary, 0, len(data))
	var prevSales float64 = 0

	for i, point := range data {
		_, err := time.Parse("2006-01-02", point.Date)
		if err != nil {
			continue
		}

		var changeRate float64
		if prevSales > 0 {
			changeRate = ((point.Sales - prevSales) / prevSales) * 100
		}

		summaries = append(summaries, models.WeeklySummary{
			WeekNumber:     i + 1,
			WeekStart:      point.Date,
			WeekEnd:        point.Date,
			TotalSales:     point.Sales,
			AverageSales:   point.Sales,
			MinSales:       point.Sales,
			MaxSales:       point.Sales,
			BusinessDays:   1,
			WeekOverWeek:   changeRate,
			StdDev:         0,
			AvgTemperature: point.Temperature,
		})

		prevSales = point.Sales
	}

	return summaries
}

// groupByMonth ãƒ‡ãƒ¼ã‚¿ã‚’æœˆæ¬¡ã§é›†ç´„
func (s *StatisticsService) groupByMonth(data []models.SalesDataPoint, startDate time.Time) []models.WeeklySummary {
	monthlyGroups := make(map[string][]models.SalesDataPoint)

	// æœˆã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
	for _, point := range data {
		date, err := time.Parse("2006-01-02", point.Date)
		if err != nil {
			continue
		}
		monthKey := date.Format("2006-01")
		monthlyGroups[monthKey] = append(monthlyGroups[monthKey], point)
	}

	// ã‚½ãƒ¼ãƒˆç”¨ã«ã‚­ãƒ¼ã‚’å–å¾—
	monthKeys := make([]string, 0, len(monthlyGroups))
	for key := range monthlyGroups {
		monthKeys = append(monthKeys, key)
	}
	sort.Strings(monthKeys)

	// ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
	summaries := make([]models.WeeklySummary, 0, len(monthKeys))
	var prevMonthSales float64 = 0

	for i, monthKey := range monthKeys {
		monthData := monthlyGroups[monthKey]
		if len(monthData) == 0 {
			continue
		}

		// æœˆã®é–‹å§‹ãƒ»çµ‚äº†æ—¥ã‚’å–å¾—
		firstDate, _ := time.Parse("2006-01-02", monthData[0].Date)
		lastDate, _ := time.Parse("2006-01-02", monthData[len(monthData)-1].Date)

		// åˆè¨ˆãƒ»å¹³å‡ãƒ»æœ€å°ãƒ»æœ€å¤§ã‚’è¨ˆç®—
		var total, avgTemp, min, max, sumSquaredDiff float64
		min = math.MaxFloat64
		max = -math.MaxFloat64

		for _, point := range monthData {
			total += point.Sales
			avgTemp += point.Temperature
			if point.Sales < min {
				min = point.Sales
			}
			if point.Sales > max {
				max = point.Sales
			}
		}

		businessDays := len(monthData)
		average := total / float64(businessDays)
		avgTemp = avgTemp / float64(businessDays)

		// å‰æœˆæ¯”ã‚’è¨ˆç®—
		var monthOverMonth float64
		if prevMonthSales > 0 {
			monthOverMonth = ((total - prevMonthSales) / prevMonthSales) * 100
		}

		// æ¨™æº–åå·®ã‚’è¨ˆç®—
		for _, point := range monthData {
			diff := point.Sales - average
			sumSquaredDiff += diff * diff
		}
		stdDev := math.Sqrt(sumSquaredDiff / float64(businessDays))

		summaries = append(summaries, models.WeeklySummary{
			WeekNumber:     i + 1,
			WeekStart:      firstDate.Format("2006-01-02"),
			WeekEnd:        lastDate.Format("2006-01-02"),
			TotalSales:     total,
			AverageSales:   average,
			MinSales:       min,
			MaxSales:       max,
			BusinessDays:   businessDays,
			WeekOverWeek:   monthOverMonth,
			StdDev:         stdDev,
			AvgTemperature: avgTemp,
		})

		prevMonthSales = total
	}

	return summaries
}

// getWeekNumber é–‹å§‹æ—¥ã‹ã‚‰ã®é€±ç•ªå·ã‚’è¨ˆç®—ï¼ˆæœˆæ›œå§‹ã¾ã‚Šï¼‰
func (s *StatisticsService) getWeekNumber(date, startDate time.Time) int {
	// æœˆæ›œæ—¥ã«èª¿æ•´
	startMonday := s.adjustToMonday(startDate)
	dateMonday := s.adjustToMonday(date)

	daysDiff := dateMonday.Sub(startMonday).Hours() / 24
	weekNum := int(daysDiff) / 7

	if weekNum < 0 {
		weekNum = 0
	}

	return weekNum
}

// adjustToMonday æ—¥ä»˜ã‚’ãã®é€±ã®æœˆæ›œæ—¥ã«èª¿æ•´
func (s *StatisticsService) adjustToMonday(date time.Time) time.Time {
	weekday := int(date.Weekday())
	if weekday == 0 { // æ—¥æ›œæ—¥
		weekday = 7
	}
	daysToMonday := weekday - 1
	return date.AddDate(0, 0, -daysToMonday)
}

// calculateWeeklySummary é€±ã”ã¨ã®ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—
func (s *StatisticsService) calculateWeeklySummary(weekNum int, weekData []models.SalesDataPoint, prevWeekSales float64) models.WeeklySummary {
	if len(weekData) == 0 {
		return models.WeeklySummary{WeekNumber: weekNum}
	}

	// é€±ã®é–‹å§‹æ—¥ãƒ»çµ‚äº†æ—¥ã‚’å–å¾—
	firstDate, _ := time.Parse("2006-01-02", weekData[0].Date)
	lastDate, _ := time.Parse("2006-01-02", weekData[len(weekData)-1].Date)

	// åˆè¨ˆãƒ»å¹³å‡ãƒ»æœ€å°ãƒ»æœ€å¤§ã‚’è¨ˆç®—
	var total, avgTemp float64
	min := math.MaxFloat64
	max := -math.MaxFloat64

	for _, point := range weekData {
		total += point.Sales
		avgTemp += point.Temperature
		if point.Sales < min {
			min = point.Sales
		}
		if point.Sales > max {
			max = point.Sales
		}
	}

	businessDays := len(weekData)
	average := total / float64(businessDays)
	avgTemp = avgTemp / float64(businessDays)

	// å‰é€±æ¯”ã‚’è¨ˆç®—
	var weekOverWeek float64
	if prevWeekSales > 0 {
		weekOverWeek = ((total - prevWeekSales) / prevWeekSales) * 100
	}

	// æ¨™æº–åå·®ã‚’è¨ˆç®—
	var sumSquaredDiff float64
	for _, point := range weekData {
		diff := point.Sales - average
		sumSquaredDiff += diff * diff
	}
	stdDev := math.Sqrt(sumSquaredDiff / float64(businessDays))

	return models.WeeklySummary{
		WeekNumber:     weekNum + 1, // 1å§‹ã¾ã‚Šã«
		WeekStart:      firstDate.Format("2006-01-02"),
		WeekEnd:        lastDate.Format("2006-01-02"),
		TotalSales:     total,
		AverageSales:   average,
		MinSales:       min,
		MaxSales:       max,
		BusinessDays:   businessDays,
		WeekOverWeek:   weekOverWeek,
		StdDev:         stdDev,
		AvgTemperature: avgTemp,
	}
}

// calculateWeeklyOverallStats å…¨ä½“çµ±è¨ˆã‚’è¨ˆç®—
func (s *StatisticsService) calculateWeeklyOverallStats(summaries []models.WeeklySummary) models.WeeklyOverallStats {
	if len(summaries) == 0 {
		return models.WeeklyOverallStats{}
	}

	// é€±æ¬¡å£²ä¸Šã‚’é›†è¨ˆ
	weeklySales := make([]float64, len(summaries))
	var total float64
	var bestWeek, worstWeek int
	var bestSales, worstSales float64 = -1, math.MaxFloat64

	for i, summary := range summaries {
		weeklySales[i] = summary.TotalSales
		total += summary.TotalSales

		if summary.TotalSales > bestSales {
			bestSales = summary.TotalSales
			bestWeek = summary.WeekNumber
		}
		if summary.TotalSales < worstSales {
			worstSales = summary.TotalSales
			worstWeek = summary.WeekNumber
		}
	}

	avgWeeklySales := total / float64(len(summaries))

	// ä¸­å¤®å€¤ã‚’è¨ˆç®—
	sortedSales := make([]float64, len(weeklySales))
	copy(sortedSales, weeklySales)
	sort.Float64s(sortedSales)

	var median float64
	mid := len(sortedSales) / 2
	if len(sortedSales)%2 == 0 {
		median = (sortedSales[mid-1] + sortedSales[mid]) / 2
	} else {
		median = sortedSales[mid]
	}

	// æ¨™æº–åå·®ã‚’è¨ˆç®—
	var sumSquaredDiff float64
	for _, sales := range weeklySales {
		diff := sales - avgWeeklySales
		sumSquaredDiff += diff * diff
	}
	stdDev := math.Sqrt(sumSquaredDiff / float64(len(weeklySales)))

	// æˆé•·ç‡ã‚’è¨ˆç®—ï¼ˆæœ€åˆã®é€± vs æœ€å¾Œã®é€±ï¼‰
	var growthRate float64
	if len(summaries) >= 2 && summaries[0].TotalSales > 0 {
		firstWeek := summaries[0].TotalSales
		lastWeek := summaries[len(summaries)-1].TotalSales
		growthRate = ((lastWeek - firstWeek) / firstWeek) * 100
	}

	// å¤‰å‹•ä¿‚æ•°ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
	var volatility float64
	if avgWeeklySales > 0 {
		volatility = stdDev / avgWeeklySales
	}

	return models.WeeklyOverallStats{
		AverageWeeklySales: avgWeeklySales,
		MedianWeeklySales:  median,
		StdDevWeeklySales:  stdDev,
		BestWeek:           bestWeek,
		WorstWeek:          worstWeek,
		GrowthRate:         growthRate,
		Volatility:         volatility,
	}
}

// analyzeWeeklyTrends é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ
func (s *StatisticsService) analyzeWeeklyTrends(summaries []models.WeeklySummary) models.WeeklyTrends {
	if len(summaries) < 2 {
		return models.WeeklyTrends{Direction: "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"}
	}

	// å‰é€±æ¯”ã®å¹³å‡ã‚’è¨ˆç®—
	var totalGrowth float64
	var positiveWeeks, negativeWeeks int
	var peakWeek, lowWeek int
	var peakSales, lowSales float64 = -1, math.MaxFloat64

	for i, summary := range summaries {
		if i > 0 { // æœ€åˆã®é€±ã¯ã‚¹ã‚­ãƒƒãƒ—
			totalGrowth += summary.WeekOverWeek
			if summary.WeekOverWeek > 0 {
				positiveWeeks++
			} else if summary.WeekOverWeek < 0 {
				negativeWeeks++
			}
		}

		if summary.TotalSales > peakSales {
			peakSales = summary.TotalSales
			peakWeek = summary.WeekNumber
		}
		if summary.TotalSales < lowSales {
			lowSales = summary.TotalSales
			lowWeek = summary.WeekNumber
		}
	}

	avgGrowth := totalGrowth / float64(len(summaries)-1)

	// ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ã‚’åˆ¤å®š
	var direction string
	var strength float64

	if avgGrowth > 2 {
		direction = "ä¸Šæ˜‡"
		strength = math.Min(avgGrowth/10, 1.0)
	} else if avgGrowth < -2 {
		direction = "ä¸‹é™"
		strength = math.Min(math.Abs(avgGrowth)/10, 1.0)
	} else {
		direction = "æ¨ªã°ã„"
		strength = 1.0 - math.Min(math.Abs(avgGrowth)/2, 1.0)
	}

	// å­£ç¯€æ€§ã®æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
	var seasonality string
	if len(summaries) >= 4 {
		// å‰åŠã¨å¾ŒåŠã§æ¯”è¼ƒ
		midPoint := len(summaries) / 2
		var firstHalfAvg, secondHalfAvg float64

		for i := 0; i < midPoint; i++ {
			firstHalfAvg += summaries[i].TotalSales
		}
		firstHalfAvg /= float64(midPoint)

		for i := midPoint; i < len(summaries); i++ {
			secondHalfAvg += summaries[i].TotalSales
		}
		secondHalfAvg /= float64(len(summaries) - midPoint)

		diff := ((secondHalfAvg - firstHalfAvg) / firstHalfAvg) * 100
		if diff > 15 {
			seasonality = "å¾ŒåŠæœŸã«éœ€è¦å¢—åŠ å‚¾å‘"
		} else if diff < -15 {
			seasonality = "å‰åŠæœŸã«éœ€è¦é›†ä¸­å‚¾å‘"
		} else {
			seasonality = "æ˜ç¢ºãªå­£ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã—"
		}
	}

	return models.WeeklyTrends{
		Direction:     direction,
		Strength:      strength,
		Seasonality:   seasonality,
		PeakWeek:      peakWeek,
		LowWeek:       lowWeek,
		AverageGrowth: avgGrowth,
	}
}

// generateWeeklyRecommendations é€±æ¬¡åˆ†æã«åŸºã¥ãæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
func (s *StatisticsService) generateWeeklyRecommendations(summaries []models.WeeklySummary, stats models.WeeklyOverallStats, trends models.WeeklyTrends) []string {
	var recommendations []string

	// ãƒˆãƒ¬ãƒ³ãƒ‰ã«åŸºã¥ãæ¨å¥¨
	switch trends.Direction {
	case "ä¸Šæ˜‡":
		recommendations = append(recommendations,
			fmt.Sprintf("ğŸ“ˆ ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå¹³å‡+%.1f%%/é€±ï¼‰ï¼šéœ€è¦å¢—åŠ ã«å‚™ãˆã¦ç”Ÿç”£èƒ½åŠ›ã®ç¢ºä¿ã‚’æ¨å¥¨", trends.AverageGrowth))
	case "ä¸‹é™":
		recommendations = append(recommendations,
			fmt.Sprintf("ğŸ“‰ ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå¹³å‡%.1f%%/é€±ï¼‰ï¼šåœ¨åº«æœ€é©åŒ–ã¨ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°å¼·åŒ–ã‚’æ¤œè¨", trends.AverageGrowth))
	case "æ¨ªã°ã„":
		recommendations = append(recommendations,
			"ğŸ“Š å®‰å®šã—ãŸéœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šç¾çŠ¶ã®ç”Ÿç”£è¨ˆç”»ã‚’ç¶­æŒã™ã‚‹ã“ã¨ã‚’æ¨å¥¨")
	}

	// ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«åŸºã¥ãæ¨å¥¨
	if stats.Volatility > 0.3 {
		recommendations = append(recommendations,
			fmt.Sprintf("âš ï¸ éœ€è¦å¤‰å‹•ãŒå¤§ãã„ã§ã™ï¼ˆå¤‰å‹•ä¿‚æ•°: %.2fï¼‰ï¼šå®‰å…¨åœ¨åº«ã®ç¢ºä¿ã‚’æ¨å¥¨", stats.Volatility))
	} else if stats.Volatility < 0.15 {
		recommendations = append(recommendations,
			"âœ… éœ€è¦ãŒå®‰å®šã—ã¦ã„ã¾ã™ï¼šJITç”Ÿç”£æ–¹å¼ã®é©ç”¨ã‚’æ¤œè¨å¯èƒ½")
	}

	// ãƒ™ã‚¹ãƒˆãƒ»ãƒ¯ãƒ¼ã‚¹ãƒˆé€±ã«åŸºã¥ãæ¨å¥¨
	if stats.BestWeek > 0 && stats.WorstWeek > 0 {
		recommendations = append(recommendations,
			fmt.Sprintf("ğŸ“… ç¬¬%dé€±ãŒæœ€é«˜ã€ç¬¬%dé€±ãŒæœ€ä½éœ€è¦ï¼šãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã§ç”Ÿç”£è¨ˆç”»ã‚’æœ€é©åŒ–", stats.BestWeek, stats.WorstWeek))
	}

	// æˆé•·ç‡ã«åŸºã¥ãæ¨å¥¨
	if stats.GrowthRate > 20 {
		recommendations = append(recommendations,
			fmt.Sprintf("ğŸš€ æœŸé–“å…¨ä½“ã§%.1f%%æˆé•·ï¼šéœ€è¦æ€¥å¢—ã«å¯¾å¿œã—ãŸä¾›çµ¦ä½“åˆ¶ã®å¼·åŒ–ãŒå¿…è¦", stats.GrowthRate))
	} else if stats.GrowthRate < -20 {
		recommendations = append(recommendations,
			fmt.Sprintf("ğŸ“Š æœŸé–“å…¨ä½“ã§%.1f%%æ¸›å°‘ï¼šéœ€è¦å›å¾©æ–½ç­–ã®ç«‹æ¡ˆã‚’æ¨å¥¨", stats.GrowthRate))
	}

	// å­£ç¯€æ€§ã«åŸºã¥ãæ¨å¥¨
	if trends.Seasonality != "æ˜ç¢ºãªå­£ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã—" {
		recommendations = append(recommendations,
			fmt.Sprintf("ğŸŒ¤ï¸ %sï¼šå­£ç¯€è¦å› ã‚’è€ƒæ…®ã—ãŸåœ¨åº«ç®¡ç†ã‚’å®Ÿæ–½", trends.Seasonality))
	}

	return recommendations
}
