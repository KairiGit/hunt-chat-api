package services

import (
	"fmt"
	"log"
	"math"
	"sort"
	"strings"
	"time"

	"hunt-chat-api/pkg/models"

	"github.com/google/uuid"
)

// StatisticsService çµ±è¨ˆåˆ†æã‚µãƒ¼ãƒ“ã‚¹
type StatisticsService struct {
	weatherService     *WeatherService
	economicService    *EconomicService
	azureOpenAIService *AzureOpenAIService
}

// NewStatisticsService æ–°ã—ã„çµ±è¨ˆåˆ†æã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
func NewStatisticsService(weatherService *WeatherService, economicService *EconomicService, azureOpenAIService *AzureOpenAIService) *StatisticsService {
	return &StatisticsService{
		weatherService:     weatherService,
		economicService:    economicService,
		azureOpenAIService: azureOpenAIService,
	}
}

// CalculateCorrelation 2ã¤ã®ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
func (s *StatisticsService) CalculateCorrelation(x, y []float64) (float64, error) {
	if len(x) != len(y) || len(x) == 0 {
		return 0, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ãªã„ã‹ã€ç©ºã§ã™")
	}

	n := float64(len(x))
	var sumX, sumY, sumXY, sumX2, sumY2 float64

	for i := 0; i < len(x); i++ {
		sumX += x[i]
		sumY += y[i]
		sumXY += x[i] * y[i]
		sumX2 += x[i] * x[i]
		sumY2 += y[i] * y[i]
	}

	numerator := n*sumXY - sumX*sumY
	denominator := math.Sqrt((n*sumX2 - sumX*sumX) * (n*sumY2 - sumY*sumY))

	if denominator == 0 {
		return 0, fmt.Errorf("åˆ†æ¯ãŒ0ã«ãªã‚Šã¾ã—ãŸï¼ˆæ¨™æº–åå·®ãŒ0ï¼‰")
	}

	return numerator / denominator, nil
}

// CalculateLaggedCorrelations x(t) vs y(t+lag) for lag in [-maxLagDays, +maxLagDays].
// Returns a sorted list by absolute correlation desc.
func (s *StatisticsService) CalculateLaggedCorrelations(xDates []string, xVals []float64, yDates []string, yVals []float64, maxLagDays int) ([]models.CorrelationResult, error) {
	if len(xDates) != len(xVals) || len(yDates) != len(yVals) {
		return nil, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“")
	}
	if len(xVals) < 5 || len(yVals) < 5 {
		return nil, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½5ç‚¹ï¼‰")
	}
	// Build map for fast lookup
	xMap := make(map[string]float64, len(xDates))
	for i, d := range xDates {
		xMap[d] = xVals[i]
	}
	yMap := make(map[string]float64, len(yDates))
	for i, d := range yDates {
		yMap[d] = yVals[i]
	}

	// Helper to shift dates by lag days
	shift := func(date string, lag int) (string, bool) {
		t, err := time.Parse("2006-01-02", date)
		if err != nil {
			return "", false
		}
		return t.AddDate(0, 0, lag).Format("2006-01-02"), true
	}

	var results []models.CorrelationResult
	for lag := -maxLagDays; lag <= maxLagDays; lag++ {
		var xs, ys []float64
		// align on x dates
		for _, d := range xDates {
			if d == "" {
				continue
			}
			shifted, ok := shift(d, lag)
			if !ok {
				continue
			}
			xv, okx := xMap[d]
			yv, oky := yMap[shifted]
			if okx && oky {
				xs = append(xs, xv)
				ys = append(ys, yv)
			}
		}
		if len(xs) >= 5 && len(xs) == len(ys) {
			r, err := s.CalculateCorrelation(xs, ys)
			if err == nil {
				p := s.CalculatePValue(r, len(xs))
				label := "lag=0"
				if lag > 0 {
					label = fmt.Sprintf("yãŒxã«å¯¾ã—ã¦+%dæ—¥é…ã‚Œ", lag)
				}
				if lag < 0 {
					label = fmt.Sprintf("yãŒxã«å¯¾ã—ã¦%dæ—¥å…ˆè¡Œ", -lag)
				}
				results = append(results, models.CorrelationResult{
					Factor:          label,
					CorrelationCoef: r,
					PValue:          p,
					SampleSize:      len(xs),
					Interpretation:  s.InterpretCorrelation(r, p),
				})
			}
		}
	}
	sort.Slice(results, func(i, j int) bool {
		return math.Abs(results[i].CorrelationCoef) > math.Abs(results[j].CorrelationCoef)
	})
	return results, nil
}

// CalculatePValue ç›¸é–¢ä¿‚æ•°ã®på€¤ã‚’è¿‘ä¼¼è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
func (s *StatisticsService) CalculatePValue(r float64, n int) float64 {
	if n < 3 {
		return 1.0
	}
	t := r * math.Sqrt(float64(n-2)) / math.Sqrt(1-r*r)
	df := float64(n - 2)
	// Use Student's t CDF for two-tailed p-value
	p := 2 * (1 - studentTCDF(math.Abs(t), df))
	if p < 0 {
		p = 0
	}
	if p > 1 {
		p = 1
	}
	return p
}

// CalculateLaggedCorrelationsWindowed runs lag scan over sliding windows across time.
// windows of size windowDays, step stepDays, returns best lag per window including p and BH-corrected p over lags.
func (s *StatisticsService) CalculateLaggedCorrelationsWindowed(xDates []string, xVals []float64, yDates []string, yVals []float64, maxLagDays int, windowDays int, stepDays int) ([]map[string]interface{}, error) {
	if len(xDates) != len(xVals) || len(yDates) != len(yVals) {
		return nil, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“")
	}
	if windowDays < 7 {
		return nil, fmt.Errorf("windowDaysã¯7ä»¥ä¸Šã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
	}
	// map for lookup
	xMap := map[string]float64{}
	for i, d := range xDates {
		xMap[d] = xVals[i]
	}
	yMap := map[string]float64{}
	for i, d := range yDates {
		yMap[d] = yVals[i]
	}

	// window loop over x timeline
	var results []map[string]interface{}
	// convert xDates to times
	var times []time.Time
	for _, d := range xDates {
		if t, err := time.Parse("2006-01-02", d); err == nil {
			times = append(times, t)
		}
	}
	if len(times) == 0 {
		return nil, fmt.Errorf("xDatesã®å½¢å¼ãŒä¸æ­£ã§ã™")
	}
	// ensure sorted
	sort.Slice(times, func(i, j int) bool { return times[i].Before(times[j]) })
	startIdx := 0
	for {
		winStart := times[startIdx]
		winEnd := winStart.AddDate(0, 0, windowDays-1)
		// collect window dates
		var wxDates []string
		for _, t := range times {
			if t.Before(winStart) || t.After(winEnd) {
				continue
			}
			wxDates = append(wxDates, t.Format("2006-01-02"))
		}
		if len(wxDates) < 5 {
			break
		}
		// sweep lags
		type lr struct {
			lag int
			r   float64
			p   float64
			n   int
		}
		var all []lr
		for lag := -maxLagDays; lag <= maxLagDays; lag++ {
			var xs, ys []float64
			for _, d := range wxDates {
				t, _ := time.Parse("2006-01-02", d)
				sd := t.AddDate(0, 0, lag).Format("2006-01-02")
				xv, okx := xMap[d]
				yv, oky := yMap[sd]
				if okx && oky {
					xs = append(xs, xv)
					ys = append(ys, yv)
				}
			}
			if len(xs) >= 5 && len(xs) == len(ys) {
				r, err := s.CalculateCorrelation(xs, ys)
				if err == nil {
					p := s.CalculatePValue(r, len(xs))
					all = append(all, lr{lag: lag, r: r, p: p, n: len(xs)})
				}
			}
		}
		if len(all) > 0 {
			// BH correction across lags in the window
			pvals := make([]float64, len(all))
			for i, a := range all {
				pvals[i] = a.p
			}
			padj := s.AdjustPValuesBH(pvals)
			// pick best by |r|
			bestIdx := 0
			for i := 1; i < len(all); i++ {
				if math.Abs(all[i].r) > math.Abs(all[bestIdx].r) {
					bestIdx = i
				}
			}
			res := map[string]interface{}{
				"window_start": winStart.Format("2006-01-02"),
				"window_end":   winEnd.Format("2006-01-02"),
				"best_lag":     all[bestIdx].lag,
				"r":            all[bestIdx].r,
				"p":            all[bestIdx].p,
				"p_adj":        padj[bestIdx],
				"n":            all[bestIdx].n,
			}
			results = append(results, res)
		}
		// advance window
		winNext := winStart.AddDate(0, 0, stepDays)
		// find index closest to winNext
		nextIdx := -1
		for i, t := range times {
			if !t.Before(winNext) {
				nextIdx = i
				break
			}
		}
		if nextIdx == -1 || nextIdx == startIdx {
			break
		}
		startIdx = nextIdx
	}
	return results, nil
}

// InterpretCorrelation ç›¸é–¢ä¿‚æ•°ã‚’äººé–“ãŒèª­ã‚ã‚‹å½¢ã§è§£é‡ˆ
func (s *StatisticsService) InterpretCorrelation(r float64, pValue float64) string {
	absR := math.Abs(r)
	strength := ""

	if absR >= 0.7 {
		strength = "å¼·ã„"
	} else if absR >= 0.4 {
		strength = "ä¸­ç¨‹åº¦ã®"
	} else if absR >= 0.2 {
		strength = "å¼±ã„"
	} else {
		strength = "ã»ã¼ç„¡ã„"
	}

	direction := "æ­£ã®"
	if r < 0 {
		direction = "è² ã®"
	}

	significance := ""
	if pValue < 0.05 {
		significance = "ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰"
	} else {
		significance = "ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ã§ã¯ãªã„ï¼‰"
	}

	return fmt.Sprintf("%s%sç›¸é–¢ %s", strength, direction, significance)
}

// PerformLinearRegression ç·šå½¢å›å¸°åˆ†æã‚’å®Ÿè¡Œ
func (s *StatisticsService) PerformLinearRegression(x, y []float64) (*models.RegressionResult, error) {
	if len(x) != len(y) || len(x) < 2 {
		return nil, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã®é•·ã•ãŒä¸€è‡´ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
	}

	n := float64(len(x))
	var sumX, sumY, sumXY, sumX2 float64

	for i := 0; i < len(x); i++ {
		sumX += x[i]
		sumY += y[i]
		sumXY += x[i] * y[i]
		sumX2 += x[i] * x[i]
	}

	// å‚¾ãï¼ˆslopeï¼‰ã®è¨ˆç®—
	slope := (n*sumXY - sumX*sumY) / (n*sumX2 - sumX*sumX)

	// åˆ‡ç‰‡ï¼ˆinterceptï¼‰ã®è¨ˆç®—
	intercept := (sumY - slope*sumX) / n

	// RÂ²ï¼ˆæ±ºå®šä¿‚æ•°ï¼‰ã®è¨ˆç®—
	meanY := sumY / n
	var ssTotal, ssResidual float64
	for i := 0; i < len(x); i++ {
		predicted := slope*x[i] + intercept
		ssTotal += (y[i] - meanY) * (y[i] - meanY)
		ssResidual += (y[i] - predicted) * (y[i] - predicted)
	}
	rSquared := 1 - (ssResidual / ssTotal)

	// äºˆæ¸¬å€¤ã®è¨ˆç®—ï¼ˆæœ€å¾Œã®xå€¤ã‚’ä½¿ç”¨ï¼‰
	lastX := x[len(x)-1]
	prediction := slope*lastX + intercept

	// ä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆRÂ²ã‚’ãƒ™ãƒ¼ã‚¹ã«ï¼‰
	confidence := rSquared

	description := fmt.Sprintf("å›å¸°å¼: y = %.2fx + %.2f (RÂ² = %.3f)", slope, intercept, rSquared)

	return &models.RegressionResult{
		Slope:       slope,
		Intercept:   intercept,
		RSquared:    rSquared,
		Prediction:  prediction,
		Confidence:  confidence,
		Description: description,
	}, nil
}

// SimpleGrangerCausality performs a simplified Granger causality test: does x help predict y?
// Returns F-stat and p-value.
func (s *StatisticsService) SimpleGrangerCausality(y []float64, x []float64, order int) (float64, float64, error) {
	T := len(y)
	if T != len(x) || T < 2*order+10 {
		return 0, 1, fmt.Errorf("ãƒ‡ãƒ¼ã‚¿é•·ä¸è¶³ã¾ãŸã¯orderéå¤§")
	}
	// Restricted model: AR(order) for y
	// Build design matrix for y: y[t] = c + a1*y[t-1] + ... + aP*y[t-P] + e
	// Full model: y[t] = c + a1*y[t-1]+...+aP*y[t-P] + b1*x[t-1]+...+bP*x[t-P] + e
	// We'll fit both and compare RSS.

	// For simplicity, we'll use a direct approach with normal equations, not intercept for AR (de-mean first).
	// De-mean
	my := 0.0
	mx := 0.0
	for i := 0; i < T; i++ {
		my += y[i]
		mx += x[i]
	}
	my /= float64(T)
	mx /= float64(T)
	Y := make([]float64, T)
	X := make([]float64, T)
	for i := 0; i < T; i++ {
		Y[i] = y[i] - my
		X[i] = x[i] - mx
	}
	// Build restricted model (only y lags)
	// rows: t=order..T-1
	n := T - order
	Xr := make([][]float64, n)
	for i := 0; i < n; i++ {
		Xr[i] = make([]float64, order)
		for lag := 0; lag < order; lag++ {
			Xr[i][lag] = Y[order-1-lag+i]
		}
	}
	YRestr := Y[order:T]
	// Build full model (y lags + x lags)
	Xf := make([][]float64, n)
	for i := 0; i < n; i++ {
		Xf[i] = make([]float64, 2*order)
		for lag := 0; lag < order; lag++ {
			Xf[i][lag] = Y[order-1-lag+i]
		}
		for lag := 0; lag < order; lag++ {
			Xf[i][order+lag] = X[order-1-lag+i]
		}
	}
	rssR, err := olsRSS(YRestr, Xr)
	if err != nil {
		return 0, 1, err
	}
	rssF, err := olsRSS(YRestr, Xf)
	if err != nil {
		return 0, 1, err
	}
	// F-test
	df1 := float64(order)
	df2 := float64(T - 2*order)
	if df2 <= 0 {
		return 0, 1, fmt.Errorf("è‡ªç”±åº¦ä¸è¶³")
	}
	F := ((rssR - rssF) / df1) / (rssF / df2)
	// Approximate p-value using F distribution tail via incomplete beta (relation with Beta)
	p := fDistSurvival(F, df1, df2)
	return F, p, nil
}

// GenerateStatisticalSummary çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
func (s *StatisticsService) GenerateStatisticalSummary(
	salesData []models.WeatherSalesData,
	regionCode string,
) (string, error) {

	if len(salesData) == 0 {
		return "", fmt.Errorf("è²©å£²ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
	}

	// åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—
	var totalSales, minSales, maxSales float64
	minSales = math.MaxFloat64
	maxSales = -math.MaxFloat64

	salesByDate := make(map[string]float64)
	for _, data := range salesData {
		totalSales += data.Sales
		if data.Sales < minSales {
			minSales = data.Sales
		}
		if data.Sales > maxSales {
			maxSales = data.Sales
		}
		salesByDate[data.Date] = data.Sales
	}

	avgSales := totalSales / float64(len(salesData))

	// æ¨™æº–åå·®ã®è¨ˆç®—
	var variance float64
	for _, data := range salesData {
		diff := data.Sales - avgSales
		variance += diff * diff
	}
	stdDev := math.Sqrt(variance / float64(len(salesData)))

	// ä¸­å¤®å€¤ã®è¨ˆç®—
	sortedSales := make([]float64, len(salesData))
	for i, data := range salesData {
		sortedSales[i] = data.Sales
	}
	sort.Float64s(sortedSales)
	median := sortedSales[len(sortedSales)/2]

	summary := fmt.Sprintf(`çµ±è¨ˆã‚µãƒãƒªãƒ¼:
- ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: %d
- å¹³å‡å£²ä¸Š: %.2f
- ä¸­å¤®å€¤: %.2f
- æ¨™æº–åå·®: %.2f
- æœ€å°å€¤: %.2f
- æœ€å¤§å€¤: %.2f
- ç·å£²ä¸Š: %.2f
`,
		len(salesData),
		avgSales,
		median,
		stdDev,
		minSales,
		maxSales,
		totalSales,
	)

	return summary, nil
}

// CreateAnalysisReport ç·åˆçš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
func (s *StatisticsService) CreateAnalysisReport(
	fileName string,
	salesData []models.WeatherSalesData,
	regionCode string,
	aiInsights string,
) (*models.AnalysisReport, error) {

	// ç›¸é–¢åˆ†æï¼ˆå¤©æ°—ãƒ‡ãƒ¼ã‚¿ï¼‰
	weatherCorrelations, err := s.AnalyzeSalesWeatherCorrelation(salesData, regionCode)
	if err != nil {
		weatherCorrelations = []models.CorrelationResult{} // ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç©ºé…åˆ—ã§ç¶™ç¶š
	}

	// ç›¸é–¢åˆ†æï¼ˆçµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ï¼‰- é…ã‚Œç›¸é–¢ã‚‚å«ã‚€
	economicCorrelations, err := s.AnalyzeSalesEconomicCorrelation(salesData, []string{"NIKKEI", "USDJPY", "WTI"}, 30)
	if err != nil {
		log.Printf("âš ï¸ çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ç›¸é–¢åˆ†æã‚¨ãƒ©ãƒ¼: %v", err)
		economicCorrelations = []models.CorrelationResult{}
	}

	// å¤©æ°—ã¨çµŒæ¸ˆã®ç›¸é–¢çµæœã‚’çµåˆ
	correlations := append(weatherCorrelations, economicCorrelations...)

	// çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ
	summary, err := s.GenerateStatisticalSummary(salesData, regionCode)
	if err != nil {
		summary = "çµ±è¨ˆã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
	}

	// å›å¸°åˆ†æï¼ˆæ°—æ¸©ã¨å£²ä¸Šï¼‰
	var regression *models.RegressionResult
	var weatherMatches int
	var dateRange string

	if len(salesData) > 0 {
		// è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ç¯„å›²ã‚’ç‰¹å®š
		var startDate, endDate time.Time
		for i, data := range salesData {
			t, err := time.Parse("2006-01-02", data.Date)
			if err != nil {
				continue
			}
			if i == 0 || t.Before(startDate) {
				startDate = t
			}
			if i == 0 || t.After(endDate) {
				endDate = t
			}
		}

		// æ—¥ä»˜ç¯„å›²ãŒç‰¹å®šã§ããªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
		if startDate.IsZero() || endDate.IsZero() {
			endDate = time.Now()
			startDate = endDate.AddDate(0, 0, -90)
		}

		dateRange = fmt.Sprintf("%s ã€œ %s", startDate.Format("2006-01-02"), endDate.Format("2006-01-02"))

		// æ°—æ¸©ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
		var temps, sales []float64
		weatherData, err := s.weatherService.GetHistoricalWeatherData(regionCode, startDate, endDate)
		if err != nil {
			log.Printf("âš ï¸ æ°—è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: %v", err)
		} else {
			log.Printf("âœ… æ°—è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: %dä»¶ (æœŸé–“: %s ã€œ %s)", len(weatherData), startDate.Format("2006-01-02"), endDate.Format("2006-01-02"))
		}

		weatherMap := make(map[string]float64)
		for _, w := range weatherData {
			weatherMap[w.Date] = w.Temperature
		}

		log.Printf("ğŸ“Š è²©å£²ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: %d, æ°—è±¡ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: %d", len(salesData), len(weatherMap))

		// æ—¥ä»˜å½¢å¼ã®è¨ºæ–­ãƒ­ã‚°ã‚’è¿½åŠ 
		if len(salesData) > 0 {
			log.Printf("ğŸ” [è¨ºæ–­] è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ä¾‹: '%s'", salesData[0].Date)
		}
		if len(weatherData) > 0 {
			log.Printf("ğŸ” [è¨ºæ–­] æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ä¾‹: '%s'", weatherData[0].Date)
		}

		for _, sale := range salesData {
			if temp, ok := weatherMap[sale.Date]; ok {
				temps = append(temps, temp)
				sales = append(sales, sale.Sales)
				weatherMatches++
			}
		}

		log.Printf("ğŸ”— ãƒãƒƒãƒãƒ³ã‚°çµæœ: %dä»¶ / %dä»¶", weatherMatches, len(salesData))

		if len(temps) >= 2 {
			regression, _ = s.PerformLinearRegression(temps, sales)
		}
	}

	// ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
	recommendations := s.generateRecommendations(correlations, regression)

	report := &models.AnalysisReport{
		ReportID:        uuid.New().String(),
		FileName:        fileName,
		AnalysisDate:    time.Now().Format(time.RFC3339),
		DataPoints:      len(salesData),
		DateRange:       dateRange,
		WeatherMatches:  weatherMatches,
		Summary:         summary,
		Correlations:    correlations,
		Regression:      regression,
		AIInsights:      aiInsights,
		Recommendations: recommendations,
	}

	return report, nil
}

// generateRecommendations åˆ†æçµæœã«åŸºã¥ã„ã¦ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
func (s *StatisticsService) generateRecommendations(
	correlations []models.CorrelationResult,
	regression *models.RegressionResult,
) []string {
	var recommendations []string

	// ç›¸é–¢åˆ†æã‹ã‚‰ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¤©æ°—ãƒ‡ãƒ¼ã‚¿ï¼‰
	for _, corr := range correlations {
		if math.Abs(corr.CorrelationCoef) > 0.5 && corr.PValue < 0.05 {
			if corr.Factor == "temperature" {
				if corr.CorrelationCoef > 0 {
					recommendations = append(recommendations, "æ°—æ¸©ãŒé«˜ã„ã»ã©å£²ä¸ŠãŒå¢—åŠ ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚å¤å­£ã®åœ¨åº«ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
				} else {
					recommendations = append(recommendations, "æ°—æ¸©ãŒä½ã„ã»ã©å£²ä¸ŠãŒå¢—åŠ ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚å†¬å­£ã®åœ¨åº«ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
				}
			}
			if corr.Factor == "humidity" {
				recommendations = append(recommendations, "æ¹¿åº¦ã¨å£²ä¸Šã«æœ‰æ„ãªç›¸é–¢ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚å¤©æ°—äºˆå ±ã¨é€£å‹•ã—ãŸåœ¨åº«ç®¡ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
			}
			// çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢
			if strings.Contains(corr.Factor, "NIKKEI") {
				if corr.CorrelationCoef > 0 {
					recommendations = append(recommendations, fmt.Sprintf("æ—¥çµŒå¹³å‡ã¨ã®æ­£ã®ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚æ ªä¾¡å‹•å‘ã‚’éœ€è¦äºˆæ¸¬ã«æ´»ç”¨ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", corr.CorrelationCoef))
				} else {
					recommendations = append(recommendations, fmt.Sprintf("æ—¥çµŒå¹³å‡ã¨ã®è² ã®ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚æ™¯æ°—å¾Œé€€æœŸã«éœ€è¦ãŒå¢—åŠ ã™ã‚‹è£½å“ç‰¹æ€§ãŒç¤ºå”†ã•ã‚Œã¾ã™ã€‚", corr.CorrelationCoef))
				}
			}
			if strings.Contains(corr.Factor, "USDJPY") {
				recommendations = append(recommendations, fmt.Sprintf("ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆï¼ˆUSD/JPYï¼‰ã¨ã®ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚è¼¸å…¥åŸææ–™ã‚„å¤–å›½äººè¦³å…‰å®¢éœ€è¦ã®å½±éŸ¿ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚", corr.CorrelationCoef))
			}
			if strings.Contains(corr.Factor, "WTI") {
				recommendations = append(recommendations, fmt.Sprintf("åŸæ²¹ä¾¡æ ¼ã¨ã®ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚è¼¸é€ã‚³ã‚¹ãƒˆã‚„æ¶ˆè²»è€…å¿ƒç†ã¸ã®å½±éŸ¿ã‚’ç›£è¦–ã—ã¦ãã ã•ã„ã€‚", corr.CorrelationCoef))
			}
		}
	}

	// é…ã‚Œç›¸é–¢ã®æ¤œå‡º
	for _, corr := range correlations {
		if strings.Contains(corr.Factor, "é…ã‚Œ") || strings.Contains(corr.Factor, "å…ˆè¡Œ") {
			if math.Abs(corr.CorrelationCoef) > 0.4 && corr.PValue < 0.05 {
				recommendations = append(recommendations, fmt.Sprintf("â±ï¸ ã‚¿ã‚¤ãƒ ãƒ©ã‚°ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: %sï¼ˆç›¸é–¢ä¿‚æ•°: %.2fï¼‰ã€‚å…ˆè¡ŒæŒ‡æ¨™ã¨ã—ã¦æ´»ç”¨ã§ãã¾ã™ã€‚", corr.Factor, corr.CorrelationCoef))
			}
		}
	}

	// å›å¸°åˆ†æã‹ã‚‰ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
	if regression != nil && regression.RSquared > 0.3 {
		recommendations = append(recommendations, fmt.Sprintf("å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¯%.1f%%ã§ã™ã€‚æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸéœ€è¦äºˆæ¸¬ãŒæœ‰åŠ¹ã§ã™ã€‚", regression.RSquared*100))
	}

	// ç›¸é–¢ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
	if len(correlations) == 0 {
		recommendations = append(recommendations, "âš ï¸ è²©å£²ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã¨å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãŒãƒãƒƒãƒã—ã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆYYYY-MM-DDå½¢å¼ã‚’æ¨å¥¨ï¼‰ã€‚")
		recommendations = append(recommendations, "ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æœŸé–“ã¨ã®æ•´åˆæ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
	}

	// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
	if len(recommendations) == 0 {
		recommendations = append(recommendations, "ã•ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿è“„ç©ã«ã‚ˆã‚Šã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚")
		recommendations = append(recommendations, "å­£ç¯€æ€§ã‚„æ›œæ—¥åŠ¹æœã‚‚è€ƒæ…®ã—ãŸå¤šå¤‰é‡è§£æã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
	}

	return recommendations
}
